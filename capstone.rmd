---
title: "Capstone Project"
author: "Abed, Nevin, and Heshan"
date: "2025-04-09"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: true
    toc_depth: 3
    theme: cosmo
    toc_float: true
---

## Potential research Questions:
+ Create a machine learning model to correctly predict the class of the network data.
+ Use different preprocessing methods to clean, deduplicate and standardize the data.
+ Explore different types of feature selection algorithms, perform a comparative study to find the most effective feature selection algorithm.
+ Test the model with k-fold cross validation and check for any discrepancies.

Suggestions for Qs improvement:
+ More Specific Prediction Goals: "How effective are different ML algorithms in detecting specific attack types (e.g., DoS vs Probe) compared to normal traffic?"
+ Protocol-Specific Analysis: "How do attack patterns and detection accuracy differ across various network protocols (TCP, UDP, ICMP)?"
+ Feature Importance Analysis: "Which network traffic features are most predictive of specific attack types, and does this vary by protocol?"
+ Imbalance Handling Comparison: "How do different class balancing techniques affect the model's ability to detect rare attack types?"
+ Anomaly Detection Focus: "Can unsupervised anomaly detection techniques effectively identify novel attack patterns not present in training data?"


```{r setup, include=FALSE}

if (rstudioapi::isAvailable()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))}

options(width = 200)

# Load libraries ---------------------------------------------------------------
# Create our function "using()" 
using <- \(pkg) {
  # if a package is not installed, install it
  if (!rlang::is_installed(pkg)) {
    install.packages(pkg, repo = "https://cloud.r-project.org")}# load the package
  library(pkg, character.only = TRUE)
}



using("data.table") # The data manipulation king
using("caret") # The ML swiss-knife - http://topepo.github.io/caret/available-models.html
using("plotly") # Beautiful interactive plots
using("ranger") # the fastest and better random forest implementation
using("xgboost") # Extreme Gradient boosting
using("imbalance") # Oversampling
using("ROSE") # Synthetic generation of new data to rebalance
using("VIM")
using("smotefamily")

```


```{r data}
data <- fread("data.csv")

```



```{r}
str(data)
View(data)


```

```{r}
colnames(data)[1:42] <- c(
  "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
  "land", "wrong_fragt", "urgent", "hot", "num_fail_login", "logged_in",
  "nu_comprom", "root_shell", "su_attempted", "num_root", "nu_file_creat",
  "nu_shells", "nu_access_files", "nu_out_cmd", "is_host_login",
  "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
  "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
  "srv_diff_h_rate", "host_count", "host_srv_count", "h_same_sr_rate",
  "h_diff_srv_rate", "h_src_port_rate", "h_srv_d_h_rate", "h_serror_rate",
  "h_sr_serror_rate", "h_rerror_rate", "h_sr_rerror_rate","class"
)

```
```{r}
as.data.frame(table(data$class))
```

```{r}
# Remove all classes with frequency <= 54
class_counts <- table(data$class)
rare_classes <- names(class_counts[class_counts <= 54])
data <- data[!data$class %in% rare_classes]
```


```{r}
any(is.na(data)) # 
colSums(is.na(data)) #
sum(complete.cases(data))
# 22469
nrow(data)
# 528603

```


```{r}
# checking for duplicates:
#sum(duplicated(data)) # 128935
#nrow(data) - sum(duplicated(data))# 399668
#remove duplicates
data <- data[!duplicated(data), ]

```


```{r}
data$class <- as.factor(data$class)
data$protocol_type <- as.factor(data$protocol_type)
data$service <- as.factor(data$service)
data$flag <- as.factor(data$flag)
```

Finding no usefullness to samples with empty class, we decided remove them.
```{r}
#data[data$class == "", ] |> head(10)
cat("Number of samples with empty class:", nrow(data[data$class == "", ]))
data <- data[data$class != '']
```


```{r}
#imputing:
for (j in names(data)) {
  if(is.numeric(data[[j]])) {
    data[is.na(get(j)), (j) := median(data[[j]], na.rm = TRUE)]}} # median imputation values is done now 

nrow(data) # 399668 after removing duplicates
#View(data)
```

Removing outliers:
Can't remove it right now because the target detection is not binary, but about 26 classes.
The following chunk is disabled for "run above" button
```{r, include=FALSE}
# removing outliers:
training_without_outliers <- copy(data)
numeric_cols <- names(data)[sapply(data, is.numeric)]
for(i in numeric_cols) { 
  for(j in unique(data$class)) {
    values <- data[class == j, get(i)]
    Q1 <- quantile(values, 0.25)
    Q3 <- quantile(values, 0.75)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    training_without_outliers <- training_without_outliers[!(
      class == j & (get(i) < lower_bound | get(i) > upper_bound))]}}

nrow(training_without_outliers)/nrow(data)
# 0.4350536
```


Handling missing char/factor values:
```{r}
# Count empty cells in each column
empty_cells <- sapply(data, function(x) sum(x == ""))
View(as.data.frame(empty_cells))
```




Checking for overlaping values:

```{r}
# Count rows with missing values in each combination of factors
missing_protocol <- data$protocol_type == ""
missing_service <- data$service == ""
missing_flag <- data$flag == ""

# Check total rows with at least one missing value
total_missing_rows <- sum(missing_protocol | missing_service | missing_flag)

# Check overlaps
protocol_service_overlap <- sum(missing_protocol & missing_service)
protocol_flag_overlap <- sum(missing_protocol & missing_flag)
service_flag_overlap <- sum(missing_service & missing_flag)
all_three_missing <- sum(missing_protocol & missing_service & missing_flag)
percent_affected <- (total_missing_rows / nrow(data)) * 100
```

Since removing ~25% of our data would be a substantial loss of information, AI recommends a combined approach:

Analyze the patterns of missingness: Check if the missing values are related to specific classes or patterns in the data. This might give insight into whether the missing data is Missing at Random (MAR) or Missing Not at Random (MNAR).
Mode imputation for rows with a single missing value: For rows where only one of the three variables is missing, use mode imputation.
Consider removal for rows with multiple missing values: For rows where two or all three variables are missing (which appears to be a much smaller subset), consider removal if those patterns don't conform to an identifiable subset of your data.




Checking percentages of missing chars in each feature:
```{r}
empty_cells_percentage <- (empty_cells / nrow(data)) * 100
#print(empty_cells_percentage)
#View(as.data.frame(empty_cells_percentage))

empty_cells_df <- data.frame(
  Column = names(empty_cells),
  Empty_Cells = empty_cells,
  Percentage = empty_cells_percentage
)
# Sort by percentage of empty cells
empty_cells_df <- empty_cells_df[order(-empty_cells_df$Percentage), ]
print(empty_cells_df)



```

```{r}
length(unique(data$flag))
length(unique(data$service))
length(unique(data$protocol_type))
```
> length(unique(data$flag))
[1] 12
> length(unique(data$service))
[1] 67
> length(unique(data$protocol_type))
[1] 4


```{r}
# Create a table of unique values for each feature
unique_values <- sapply(data, function(x) length(unique(x)))
unique_values_df <- data.frame(
  Feature = names(unique_values),
  Unique_Values = unique_values)

# Sort by number of unique values
unique_values_df <- unique_values_df[order(-unique_values_df$Unique_Values), ]

# Print the table
print(unique_values_df)

# View in a more readable format
View(unique_values_df)
```


## Handling Class Imbalance
```{r}
# Set a balanced target for each class
target_samples <- 2000  # We'll aim for 2000 samples per class

# Create a balanced dataset using both undersampling and SMOTE
balanced_data <- data.table()       

# First, undersample the majority classes
for (cls in unique(data$class[data$class != ""])) {
  class_data <- data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    # Undersample the majority classes
    set.seed(123)  # For reproducibility
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data <- rbind(balanced_data, balanced_class_data)
  } else {
    # Keep all samples from minority classes
    balanced_data <- rbind(balanced_data, class_data)
  }
}

# Now handle the minority classes using SMOTE
# First convert to data.frame (SMOTE functions expect data.frames)
balanced_df <- as.data.frame(balanced_data)

# We need to convert factor variables to numeric for SMOTE
# Store original factor columns and their levels
factor_cols <- names(balanced_df)[sapply(balanced_df, is.factor)]
factor_levels <- lapply(balanced_df[, factor_cols, drop = FALSE], levels)

# Convert factors to numeric
for (col in factor_cols) {
  balanced_df[[col]] <- as.numeric(balanced_df[[col]])
}

# Apply SMOTE to oversample minority classes
set.seed(123)

# Create a binary classifier for each minority class
for (cls in unique(balanced_data$class[balanced_data$class != ""])) {
  class_count <- sum(balanced_data$class == cls)
  
  if (class_count < target_samples && class_count > 0) {
    # Create a binary classification problem for this class
    temp_df <- balanced_df
    temp_df$target <- ifelse(temp_df$class == as.numeric(factor(cls)), 1, 0)
    
    # Count samples in this class
    n_minority <- sum(temp_df$target == 1)
    
    if (n_minority >= 5) {  # SMOTE needs at least a few samples to work with
      # Determine how many synthetic samples to create
      n_synthetic <- target_samples - n_minority
      
      # Apply SMOTE with dup_size parameter (ratio of synthetic to original samples)
      dup_size <- ceiling(n_synthetic / n_minority)
      
      # Select only the rows for this class to apply SMOTE
      minority_data <- temp_df[temp_df$target == 1, ]
      
      # Only proceed if we have enough samples
      if (nrow(minority_data) >= 5) {
        # Create synthetic samples using SMOTE
        smote_result <- try(SMOTE(
          X = minority_data[, !names(minority_data) %in% c("target", "class")],
          target = minority_data$target,
          K = min(5, nrow(minority_data) - 1),  # Use fewer neighbors if needed
          dup_size = dup_size
        ), silent = TRUE)
        
        if (!inherits(smote_result, "try-error")) {
          # Extract the synthetic samples
          synthetic_samples <- smote_result$syn_data
          
          # Add back the class column
          synthetic_samples$class <- as.numeric(factor(cls))
          
          # Remove the target column if it exists
          if ("target" %in% names(synthetic_samples)) {
            synthetic_samples$target <- NULL
          }
          
          # Limit the number of synthetic samples to what we need
          if (nrow(synthetic_samples) > n_synthetic) {
            synthetic_samples <- synthetic_samples[1:n_synthetic, ]
          }
          
          # Add the synthetic samples to our balanced dataframe
          if (nrow(synthetic_samples) > 0) {
            balanced_df <- rbind(balanced_df[balanced_df$class != as.numeric(factor(cls)), ],
                                rbind(balanced_df[balanced_df$class == as.numeric(factor(cls)), ],
                                     synthetic_samples))
          }
        }
      }
    }
  }
}

# Remove the target column if it exists
if ("target" %in% names(balanced_df)) {
  balanced_df$target <- NULL
}

# Convert back to factors using the original levels
for (col in factor_cols) {
  balanced_df[[col]] <- factor(balanced_df[[col]], levels = 1:length(factor_levels[[col]]), 
                              labels = factor_levels[[col]])
}

# Convert back to data.table
balanced_data <- as.data.table(balanced_df)

# Check the new class distribution
print("Balanced class distribution:")
balanced_class_table <- as.data.frame(table(balanced_data$class))
print(balanced_class_table)

# Verify the total number of samples
cat("Original dataset size:", nrow(data), "\n")
cat("Balanced dataset size:", nrow(balanced_data), "\n")

# Alternative approach: Using simple random oversampling for smaller classes
# This is a simpler approach that might work better in your case

# Create a new data.table for the simpler approach
balanced_data_simple <- data.table()

set.seed(123)  # For reproducibility

for (cls in unique(data$class[data$class != ""])) {
  class_data <- data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    # Undersample the majority classes
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples > 0 && n_samples < target_samples) {
    # Random oversampling with replacement for minority classes
    oversampled_indices <- sample(1:n_samples, target_samples, replace = TRUE)
    balanced_class_data <- class_data[oversampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples == target_samples) {
    # Keep as is if already at target
    balanced_data_simple <- rbind(balanced_data_simple, class_data)
  }
}

# Check the new class distribution using the simple approach
print("Balanced class distribution (simple approach):")
balanced_class_table_simple <- as.data.frame(table(balanced_data_simple$class))
print(balanced_class_table_simple)

# Verify the total number of samples
cat("Balanced dataset size (simple approach):", nrow(balanced_data_simple), "\n")

# Use the simpler balanced dataset for further modeling
balanced_data <- balanced_data_simple
```

Before Balancing:

              Var1   Freq
1                       0
2         b'back.'   2148
3      b'ipsweep.'   1221
4      b'neptune.' 104990
5         b'nmap.'    232
6       b'normal.'  95611
7          b'pod.'    255
8    b'portsweep.'    989
9        b'satan.'   1536
10       b'smurf.' 154486
11    b'teardrop.'    966
12 b'warezclient.'    995



After Balancing:

              Var1 Freq
1                     0
2         b'back.' 2000
3      b'ipsweep.' 2000
4      b'neptune.' 2000
5         b'nmap.' 2000
6       b'normal.' 2000
7          b'pod.' 2000
8    b'portsweep.' 2000
9        b'satan.' 2000
10       b'smurf.' 2000
11    b'teardrop.' 2000
12 b'warezclient.' 2000


```{r}
# Before balancing
before_plot <- ggplot(as.data.frame(table(data$class)), aes(x = reorder(Var1, -Freq), y = Freq)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Class Distribution Before Balancing",
       x = "Class", y = "Frequency")

# After balancing
after_plot <- ggplot(balanced_class_table_simple, aes(x = reorder(Var1, -Freq), y = Freq)) +
  geom_bar(stat = "identity", fill = "forestgreen") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Class Distribution After Balancing",
       x = "Class", y = "Frequency")

before_plot
after_plot
```




## Preparing splits + Balancing data 

```{r}

# Step 1: Split BEFORE balancing
# ------------------------------
set.seed(42)
train_index <- createDataPartition(data$class, p = 0.7, list = FALSE)
train_data <- data[train_index]
test_data <- data[-train_index]

# ------------------------------
# Step 2: Apply your balancing code to train_data
# ------------------------------
# Set a balanced target for each class
target_samples <- 2000  # We'll aim for 2000 samples per class

# Create a balanced dataset using both undersampling and oversampling (simpler path)
balanced_data_simple <- data.table()

set.seed(123)  # For reproducibility

for (cls in unique(train_data$class[train_data$class != ""])) {
  class_data <- train_data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples > 0 && n_samples < target_samples) {
    oversampled_indices <- sample(1:n_samples, target_samples, replace = TRUE)
    balanced_class_data <- class_data[oversampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples == target_samples) {
    balanced_data_simple <- rbind(balanced_data_simple, class_data)
  }
}

# Use this as training data
balanced_data <- balanced_data_simple

# ------------------------------
# Step 3: Check class distributions
# ------------------------------
cat("Balanced training set class distribution:\n")
print(as.data.frame(table(balanced_data$class)))

cat("Unbalanced test set class distribution:\n")
print(as.data.frame(table(test_data$class)))
```

## Feature importance
```{r}
# Create a copy of the balanced data
feature_selection_data <- copy(balanced_data)

# Add a random feature as a baseline for importance comparison
set.seed(123)
feature_selection_data[, random := runif(nrow(feature_selection_data), 1, 100)]

# Train a quick random forest to get feature importance
fs_model <- ranger(
  formula = class ~ ., 
  data = feature_selection_data,
  importance = 'impurity',
  num.trees = 100,  # Using fewer trees for quicker analysis
  mtry = floor(sqrt(ncol(feature_selection_data) - 1)),
  verbose = TRUE
)

# Extract feature importance
imp <- importance(fs_model)
imp_df <- data.frame(Feature = names(imp), Importance = imp)

# Sort by importance
imp_df <- imp_df[order(-imp_df$Importance), ]

# Create interactive bar plot of feature importance
importance_plot <- plot_ly(data = imp_df, 
                          x = ~reorder(Feature, Importance), 
                          y = ~Importance, 
                          type = "bar",
                          marker = list(color = "steelblue")) %>%
  layout(title = "Feature Importance for Network Intrusion Detection",
         xaxis = list(title = "Features", categoryorder = "total descending"),
         yaxis = list(title = "Importance Score"))

# Display the plot
importance_plot
```

```{r}
## Feature Selection Implementation ----
print("Implementing feature selection based on importance")

# Get feature names in order of importance (already in imp_df)



# Select features with importance above certain threshold
# From the plot, we can see importance drops significantly after around 400-500
importance_threshold <- 400
selected_features_threshold <- as.character(imp_df$Feature[imp_df$Importance > importance_threshold])
cat("\nFeatures with importance > ", importance_threshold, ":\n")
print(selected_features_threshold)


selected_features <- selected_features_threshold

# Create a new dataset with only selected features plus the class column
print(paste("Creating new dataset with", length(selected_features), "selected features"))
balanced_data_selected <- balanced_data[, c(selected_features, "class"), with = FALSE]

# Verify the dimensions
cat("Original dataset dimensions:", dim(balanced_data)[1], "rows and", dim(balanced_data)[2], "columns\n")
cat("Selected dataset dimensions:", dim(balanced_data_selected)[1], "rows and", dim(balanced_data_selected)[2], "columns\n")

# Also prepare test data with same features for consistent evaluation
test_data_selected <- test_data[, c(selected_features, "class"), with = FALSE]

# Update our dataset references to use the selected features
# Comment this line if you want to keep using all features
balanced_data <- balanced_data_selected
test_data <- test_data_selected


```


Based on feature selection, we keep the following features and train our model accordingly. We decided to select the treshold for importance at 400 as seen in the plot.
 "src_bytes"       "wrong_fragt"     "srv_count"       "count"           "service"         "h_srv_d_h_rate"  "same_srv_rate"   "h_src_port_rate" "logged_in"       "host_srv_count"
"rerror_rate"     "dst_bytes"       "protocol_type"   "nu_comprom"      "diff_srv_rate"   "host_count"      "flag"


## Random Forest + Evaluation

```{r}
# Train the Random Forest
# ------------------------------
print("Training Random Forest model...")
model_rf <- ranger(
  formula = class ~ ., 
  data = balanced_data,
  importance = 'impurity',
  num.trees = 500,
  mtry = floor(sqrt(ncol(balanced_data) - 1)),
  min.node.size = 5,
  verbose = TRUE
)

# ------------------------------
# Evaluate on test set
# ------------------------------
pred_rf <- predict(model_rf, data = test_data)
predictions <- pred_rf$predictions

conf_matrix <- table(Predicted = predictions, Actual = test_data$class)
print("Confusion Matrix:")
print(conf_matrix)

accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Overall Accuracy:", round(accuracy, 4)))

# ------------------------------
# Per-class metrics
# ------------------------------
calculate_class_metrics <- function(conf_mat) {
  n_classes <- nrow(conf_mat)
  class_names <- rownames(conf_mat)
  
  metrics <- data.frame(
    Class = class_names,
    Precision = numeric(n_classes),
    Recall = numeric(n_classes),
    F1_Score = numeric(n_classes),
    Support = numeric(n_classes)
  )
  
  for (i in 1:n_classes) {
    tp <- conf_mat[i, i]
    fp <- sum(conf_mat[, i]) - tp
    fn <- sum(conf_mat[i, ]) - tp
    precision <- if(tp + fp > 0) tp / (tp + fp) else 0
    recall <- if(tp + fn > 0) tp / (tp + fn) else 0
    f1 <- if(precision + recall > 0) 2 * precision * recall / (precision + recall) else 0
    
    metrics$Precision[i] <- precision
    metrics$Recall[i] <- recall
    metrics$F1_Score[i] <- f1
    metrics$Support[i] <- sum(conf_mat[i, ])
  }
  
  return(metrics)
}

class_metrics <- calculate_class_metrics(conf_matrix)
print("Per-class metrics:")
print(class_metrics)

macro_avg <- colMeans(class_metrics[, c("Precision", "Recall", "F1_Score")])
print("Macro Average metrics:")
print(macro_avg)
```

Our Random Forest model achieves exceptional performance on the network intrusion detection task with 99.6% overall accuracy, demonstrating robust classification capabilities across various attack types. The macro-averaged metrics (Precision: 0.907, Recall: 0.842, F1-Score: 0.868) confirm strong generalization ability despite class imbalance. Several attack classes show near-perfect detection, including neptune (F1: 0.999), smurf (F1: 0.9999), normal traffic (F1: 0.993), teardrop (F1: 0.997), and pod (F1: 0.981). Moderate performance is observed for back (F1: 0.983), satan (F1: 0.968), and portsweep (F1: 0.939) attacks. The most significant challenge appears with warezclient attacks, which despite perfect precision suffer from lower recall (0.535), with many instances misclassified as normal traffic. Similarly, ipsweep (Recall: 0.879) and nmap (Precision/Recall: 0.928) show room for improvement. The feature selection approach has clearly identified the most informative network traffic characteristics, enabling the model to effectively distinguish between normal traffic and various attack types, though future refinements should focus on improving detection of warezclient attacks to further enhance the system's security capabilities.



