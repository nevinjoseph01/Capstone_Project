---
title: "Decision Trees Assignment"
author: "Abed, Nevin, and Heshan"
date: "2025-04-09"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: true
    toc_depth: 3
    theme: cosmo
    toc_float: true
---


```{r setup, include=FALSE}

if (rstudioapi::isAvailable()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))}

options(width = 200)

# Load libraries ---------------------------------------------------------------
# Create our function "using()" 
using <- \(pkg) {
  # if a package is not installed, install it
  if (!rlang::is_installed(pkg)) {
    install.packages(pkg, repo = "https://cloud.r-project.org")}# load the package
  library(pkg, character.only = TRUE)
}



using("data.table") # The data manipulation king
using("caret") # The ML swiss-knife - http://topepo.github.io/caret/available-models.html
using("plotly") # Beautiful interactive plots
using("ranger") # the fastest and better random forest implementation
using("xgboost") # Extreme Gradient boosting
using("imbalance") # Oversampling
using("ROSE") # Synthetic generation of new data to rebalance
using("VIM")

```


```{r data}
data <- fread("data.csv")

```



```{r}
str(data)
View(data)


```

```{r}
colnames(data)[1:42] <- c(
  "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
  "land", "wrong_fragt", "urgent", "hot", "num_fail_login", "logged_in",
  "nu_comprom", "root_shell", "su_attempted", "num_root", "nu_file_creat",
  "nu_shells", "nu_access_files", "nu_out_cmd", "is_host_login",
  "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
  "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
  "srv_diff_h_rate", "host_count", "host_srv_count", "h_same_sr_rate",
  "h_diff_srv_rate", "h_src_port_rate", "h_srv_d_h_rate", "h_serror_rate",
  "h_sr_serror_rate", "h_rerror_rate", "h_sr_rerror_rate","class"
)

```
```{r}
as.data.frame(table(data$class))
```
```{r}
any(is.na(data)) # 
colSums(is.na(data)) #
sum(complete.cases(data))
# 22469
nrow(data)
# 528603

```


```{r}
# checking for duplicates:
#sum(duplicated(data)) # 128935
#nrow(data) - sum(duplicated(data))# 399668
#remove duplicates
data <- data[!duplicated(data), ]

```
```{r}

#data.frame(names(data))

```

```{r}
data$class <- as.factor(data$class)
data$protocol_type <- as.factor(data$protocol_type)
data$service <- as.factor(data$service)
data$flag <- as.factor(data$flag)




```

Finding no usefullness to samples with empty class, we decided remove them.
```{r}
data[data$class == "", ] |> head(10)
nrow(data[data$class == "", ]) # 36061
data <- data[data$class != '']
```


```{r}

#imputing:
for (j in names(data)) {
  if(is.numeric(data[[j]])) {
    data[is.na(get(j)), (j) := median(data[[j]], na.rm = TRUE)]}} # fucking median imputation values is done now 

nrow(data) # 399668 after removing duplicates
#View(data)
```

Removing outliers:
Can't remove it right now because the target detection is not binary, but about 26 classes.
The following chunk is disabled for "run above" button
```{r, include=FALSE}
# removing outliers:
training_without_outliers <- copy(data)
numeric_cols <- names(data)[sapply(data, is.numeric)]
for(i in numeric_cols) { 
  for(j in unique(data$class)) {
    values <- data[class == j, get(i)]
    Q1 <- quantile(values, 0.25)
    Q3 <- quantile(values, 0.75)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    training_without_outliers <- training_without_outliers[!(
      class == j & (get(i) < lower_bound | get(i) > upper_bound))]}}

nrow(training_without_outliers)/nrow(data)
# 0.4350536
```



Handling missing char/factor values:
```{r}
# Count empty cells in each column
empty_cells <- sapply(data, function(x) sum(x == ""))
View(as.data.frame(empty_cells))
```




Checking for overlaping values:

```{r}
# Count rows with missing values in each combination of factors
missing_protocol <- data$protocol_type == ""
missing_service <- data$service == ""
missing_flag <- data$flag == ""

# Check total rows with at least one missing value
total_missing_rows <- sum(missing_protocol | missing_service | missing_flag)

# Check overlaps
protocol_service_overlap <- sum(missing_protocol & missing_service)
protocol_flag_overlap <- sum(missing_protocol & missing_flag)
service_flag_overlap <- sum(missing_service & missing_flag)
all_three_missing <- sum(missing_protocol & missing_service & missing_flag)
percent_affected <- (total_missing_rows / nrow(data)) * 100
```

Since removing ~25% of our data would be a substantial loss of information, AI recommends a combined approach:

Analyze the patterns of missingness: Check if the missing values are related to specific classes or patterns in the data. This might give insight into whether the missing data is Missing at Random (MAR) or Missing Not at Random (MNAR).
Mode imputation for rows with a single missing value: For rows where only one of the three variables is missing, use mode imputation.
Consider removal for rows with multiple missing values: For rows where two or all three variables are missing (which appears to be a much smaller subset), consider removal if those patterns don't conform to an identifiable subset of your data.




Checking percentages of missing chars in each feature:
```{r}
empty_cells_percentage <- (empty_cells / nrow(data)) * 100
#print(empty_cells_percentage)
#View(as.data.frame(empty_cells_percentage))

empty_cells_df <- data.frame(
  Column = names(empty_cells),
  Empty_Cells = empty_cells,
  Percentage = empty_cells_percentage
)
print(empty_cells_df)


```
```{r}

# Sort by percentage of empty cells
empty_cells_df <- empty_cells_df[order(-empty_cells_df$Percentage), ]
print(empty_cells_df)


```


```{r}
length(unique(data$flag))
length(unique(data$service))
length(unique(data$protocol_type))
```
> length(unique(data$flag))
[1] 13
> length(unique(data$service))
[1] 68
> length(unique(data$protocol_type))
[1] 5


```{r}
# Create a table of unique values for each feature
unique_values <- sapply(data, function(x) length(unique(x)))
unique_values_df <- data.frame(
  Feature = names(unique_values),
  Unique_Values = unique_values)

# Sort by number of unique values
unique_values_df <- unique_values_df[order(-unique_values_df$Unique_Values), ]

# Print the table
print(unique_values_df)

# View in a more readable format
View(unique_values_df)
```

```{r}

```

## Creating Two Sets for Handling Missing Categorical Values

### Dataset 1: Mode Imputation for Empty Character/Factor Values

```{r}
# Create a copy of the original data
data_mode_imputed <- copy(data)

# Calculate modes for categorical variables with empty values
protocol_mode <- names(sort(table(data$protocol_type[data$protocol_type != ""]), decreasing = TRUE))[1]
service_mode <- names(sort(table(data$service[data$service != ""]), decreasing = TRUE))[1]
flag_mode <- names(sort(table(data$flag[data$flag != ""]), decreasing = TRUE))[1]

# Display the modes
cat("Mode values for imputation:\n")
cat("Protocol type mode:", protocol_mode, "\n")
cat("Service mode:", service_mode, "\n")
cat("Flag mode:", flag_mode, "\n")

# Impute empty values with modes
data_mode_imputed[protocol_type == "", protocol_type := protocol_mode]
data_mode_imputed[service == "", service := service_mode]
data_mode_imputed[flag == "", flag := flag_mode]

# Verify that imputation worked
cat("\nVerifying imputation - count of empty values after imputation:\n")
cat("Protocol type:", sum(data_mode_imputed$protocol_type == ""), "\n")
cat("Service:", sum(data_mode_imputed$service == ""), "\n")
cat("Flag:", sum(data_mode_imputed$flag == ""), "\n")

cat("\nRows in mode-imputed dataset:", nrow(data_mode_imputed), "\n")
```

### Dataset 2: Removal of Rows with Empty Character/Factor Values

```{r}
# Create a dataset with rows removed where any of the categorical features have empty values
data_removed <- data[protocol_type != "" & service != "" & flag != "", ]

# Check the number of rows removed
cat("Original dataset rows:", nrow(data), "\n")
cat("Rows after removal:", nrow(data_removed), "\n")
cat("Percentage of data retained:", round(nrow(data_removed)/nrow(data) * 100, 2), "%\n")
```

## Addressing Multi-Class Classification and Potential Overfitting

```{r}
# Check class distribution
class_counts <- table(data$class)
n_classes <- length(class_counts)

cat("Number of classes:", n_classes, "\n")
cat("Class distribution (top 10):\n")
print(sort(class_counts, decreasing = TRUE)[1:10])

# Calculate class imbalance metrics
max_class_pct <- max(class_counts) / sum(class_counts) * 100
min_class_pct <- min(class_counts) / sum(class_counts) * 100

cat("\nClass imbalance metrics:\n")
cat("Most common class: ", round(max_class_pct, 2), "% of data\n", sep="")
cat("Least common class: ", round(min_class_pct, 4), "% of data\n", sep="")
cat("Ratio between most and least common: ", round(max(class_counts)/min(class_counts), 2), "\n", sep="")
```

## Discussion on Model Selection and Overfitting

Random Forest is indeed appropriate for multi-class classification problems, but the extremely high accuracy (0.994/0.999) suggests potential issues:

1. **Data leakage**: Some features might directly reveal the class label
2. **Overfitting**: The model could be memorizing the training data rather than learning generalizable patterns
3. **Class imbalance**: High accuracy might be achieved by simply predicting the majority class
4. **Data redundancy**: Despite removing duplicates, highly correlated features might exist

Recommended next steps:

```{r, eval=FALSE}
# Suggestions for addressing potential overfitting:

# 1. Feature importance analysis to identify potential data leakage
# Already implemented in your current code

# 2. Learning curves to diagnose overfitting
# You might add code like:
# plot_learning_curve <- function(...)

# 3. Confusion matrix analysis by class
# Already implemented in your code

# 4. Try simpler models first
# For example, decision trees or logistic regression

# 5. Consider stratified sampling for class imbalance
# trainIndex <- createDataPartition(data$class, p=0.7, list=FALSE, times=1)
```

Random Forest Model + Comparing Training vs Testing

```{r, include=FALSE}}
# First model: Using mode-imputed data
set.seed(42)
shuffled_indices <- sample(1:nrow(data_mode_imputed))
data_mode_imputed <- data_mode_imputed[shuffled_indices]
train_index_imputed <- createDataPartition(data_mode_imputed$class, p = 0.7, list = FALSE)
train_data_imputed <- data_mode_imputed[train_index_imputed]
test_data_imputed <- data_mode_imputed[-train_index_imputed]

# Train Random Forest on imputed data
model_rf_imputed <- ranger(
  class ~ ., 
  data = train_data_imputed,
  importance = 'impurity',
  num.trees = 100,
  mtry = floor(sqrt(ncol(data_mode_imputed) - 1)),
  min.node.size = 5,
  verbose = TRUE
)

# Predict on imputed test data
pred_rf_imputed <- predict(model_rf_imputed, data = test_data_imputed)
table_result_imputed <- table(Predicted = pred_rf_imputed$predictions, Actual = test_data_imputed$class)
accuracy_imputed <- sum(diag(table_result_imputed)) / sum(table_result_imputed)

# Get feature importance for imputed model
importance_df_imputed <- data.frame(
  Feature = names(model_rf_imputed$variable.importance),
  Importance = model_rf_imputed$variable.importance
)
importance_df_imputed <- importance_df_imputed[order(-importance_df_imputed$Importance), ]

# Second model: Using data with rows removed
set.seed(42)
train_index_removed <- createDataPartition(data_removed$class, p = 0.7, list = FALSE)
train_data_removed <- data_removed[train_index_removed]
test_data_removed <- data_removed[-train_index_removed]

# Train Random Forest on data with rows removed
model_rf_removed <- ranger(
  class ~ ., 
  data = train_data_removed,
  importance = 'impurity',
  num.trees = 100,
  mtry = floor(sqrt(ncol(data_removed) - 1)),
  min.node.size = 5,
  verbose = TRUE
)

# Predict on test data with rows removed
pred_rf_removed <- predict(model_rf_removed, data = test_data_removed)
table_result_removed <- table(Predicted = pred_rf_removed$predictions, Actual = test_data_removed$class)
accuracy_removed <- sum(diag(table_result_removed)) / sum(table_result_removed)

# Get feature importance for removed-rows model
importance_df_removed <- data.frame(
  Feature = names(model_rf_removed$variable.importance),
  Importance = model_rf_removed$variable.importance
)
importance_df_removed <- importance_df_removed[order(-importance_df_removed$Importance), ]

# Compare the results
cat("Accuracy with mode imputation:", accuracy_imputed, "\n")
cat("Accuracy with rows removed:", accuracy_removed, "\n")

# Compare top 10 important features from both approaches
top_features_comparison <- data.frame(
  Rank = 1:10,
  Imputed_Feature = importance_df_imputed$Feature[1:10],
  Imputed_Importance = importance_df_imputed$Importance[1:10],
  Removed_Feature = importance_df_removed$Feature[1:10],
  Removed_Importance = importance_df_removed$Importance[1:10]
)
print(top_features_comparison)

# Original model for reference
# Split data into training and testing sets
# set.seed(42)
# train_index <- createDataPartition(data$class, p = 0.7, list = FALSE)
# train_data <- data[train_index]
# test_data <- data[-train_index]

# # Train Random Forest model with ranger
# model_rf <- ranger(
#   class ~ ., 
#   data = train_data,
#   importance = 'impurity',  # For feature importance
#   num.trees = 100,          # Start with fewer trees for speed
#   mtry = floor(sqrt(ncol(data) - 1)), # Default RF formula
#   min.node.size = 5,        # Smaller for better precision
#   verbose = TRUE            # Track progress
# )

# # Making predictions
# pred_rf <- predict(model_rf, data = test_data)


# # NOTE: I GET ERROR IN CONFUSION MATRIX
# # DOESNT WORK: Error in `[.default`(data, , pos) : subscript out of bounds
# # confusionMatrix(pred_rf$predictions, test_data$class)


# #SO WE USE THIS INSTEAD!
# # Alternative method to evaluate predictions
# table_result <- table(Predicted = pred_rf$predictions, Actual = test_data$class)
# print(table_result)

# # Calculate accuracy manually
# # Predict on training data
# train_pred_rf <- predict(model_rf, data = train_data)

# # Create confusion matrix for training data
# train_table_result <- table(Predicted = train_pred_rf$predictions, Actual = train_data$class)

# # Calculate training accuracy manually
# train_accuracy <- sum(diag(train_table_result)) / sum(train_table_result)


# # Compare training and testing accuracy
# print(paste("Training Accuracy:", round(train_accuracy, 4)))
# print(paste("Testing Accuracy:", round(accuracy, 4)))

# # Get feature importance
# importance_df <- data.frame(
#   Feature = names(model_rf$variable.importance),
#   Importance = model_rf$variable.importance
# )
# importance_df <- importance_df[order(-importance_df$Importance), ]
# head(importance_df, 10)  # Show top 10 features

```

## Comparison of Model Performance: Imputed vs. Removed

```{r}
# Create a comparison table of the two approaches
performance_comparison <- data.frame(
  Metric = c("Dataset Size", "Accuracy", "Top Feature"),
  Imputed_Data = c(nrow(data_mode_imputed), 
                   round(accuracy_imputed, 4), 
                   as.character(importance_df_imputed$Feature[1])),
  Removed_Data = c(nrow(data_removed), 
                  round(accuracy_removed, 4), 
                  as.character(importance_df_removed$Feature[1]))
)

# Print the comparison
print(performance_comparison)

# Compare class distributions in both datasets
class_dist_imputed <- prop.table(table(data_mode_imputed$class)) * 100
class_dist_removed <- prop.table(table(data_removed$class)) * 100

# Get top 5 classes
top_classes <- names(sort(table(data$class), decreasing = TRUE)[1:5])

class_dist_comparison <- data.frame(
  Class = top_classes,
  Imputed_Pct = round(class_dist_imputed[top_classes], 2),
  Removed_Pct = round(class_dist_removed[top_classes], 2),
  Difference = round(class_dist_imputed[top_classes] - class_dist_removed[top_classes], 2)
)

print(class_dist_comparison)
```

## Evaluation of Random Forest for Multi-Class Classification

For a 25-category classification problem, Random Forest is generally a good choice because:

1. It naturally handles multi-class problems without requiring one-vs-all transformations
2. It can capture complex, non-linear relationships between features
3. It provides feature importance measures
4. It's robust to irrelevant features

However, the extremely high accuracy (0.99+) in both training and testing suggests potential issues:

1. **Data leakage**: Some features might be too predictive, implying information leakage
2. **Class imbalance**: Very high accuracy might come from predicting dominant classes well
3. **Overfitting**: The model might be capturing noise or memorizing patterns specific to this dataset

## K-fold Cross-Validation with Both Datasets

```{r}
# K-fold cross-validation for mode-imputed data
k <- 5
folds_imputed <- createFolds(data_mode_imputed$class, k = k)
cv_results_imputed <- data.frame()

for (i in 1:k) {
  # Split data
  test_indices <- folds_imputed[[i]]
  cv_train <- data_mode_imputed[-test_indices]
  cv_test <- data_mode_imputed[test_indices]
  
  # Train model (with fewer trees for speed)
  cv_model <- ranger(class ~ ., data = cv_train, num.trees = 50)
  
  # Predict
  cv_pred <- predict(cv_model, data = cv_test)
  cv_acc <- sum(cv_pred$predictions == cv_test$class) / nrow(cv_test)
  
  # Store results
  cv_results_imputed <- rbind(cv_results_imputed, data.frame(Fold = i, Accuracy = cv_acc))
}

# Calculate mean and standard deviation for imputed data
mean_acc_imputed <- mean(cv_results_imputed$Accuracy)
sd_acc_imputed <- sd(cv_results_imputed$Accuracy)

# K-fold cross-validation for removed-rows data
folds_removed <- createFolds(data_removed$class, k = k)
cv_results_removed <- data.frame()

for (i in 1:k) {
  # Split data
  test_indices <- folds_removed[[i]]
  cv_train <- data_removed[-test_indices]
  cv_test <- data_removed[test_indices]
  
  # Train model (with fewer trees for speed)
  cv_model <- ranger(class ~ ., data = cv_train, num.trees = 50)
  
  # Predict
  cv_pred <- predict(cv_model, data = cv_test)
  cv_acc <- sum(cv_pred$predictions == cv_test$class) / nrow(cv_test)
  
  # Store results
  cv_results_removed <- rbind(cv_results_removed, data.frame(Fold = i, Accuracy = cv_acc))
}

# Calculate mean and standard deviation for removed-rows data
mean_acc_removed <- mean(cv_results_removed$Accuracy)
sd_acc_removed <- sd(cv_results_removed$Accuracy)

# Compare cross-validation results
cv_comparison <- data.frame(
  Dataset = c("Mode Imputed", "Rows Removed"),
  Mean_Accuracy = c(mean_acc_imputed, mean_acc_removed),
  Std_Deviation = c(sd_acc_imputed, sd_acc_removed)
)

print(cv_comparison)
```

## Discussion and Next Steps

Based on the research questions in the README and the results of our analysis, here are some recommendations:

1. **Feature engineering**: Create new features that might better capture attack patterns
2. **Dimensionality reduction**: Consider PCA or feature selection to reduce redundancy
3. **Class rebalancing**: Try SMOTE or other techniques for the minority classes
4. **Different model architectures**: Try gradient boosting, neural networks, or ensemble methods
5. **Early stopping**: Use validation sets to prevent overfitting
6. **Anomaly detection**: Consider treating this as an anomaly detection problem rather than classification

