---
title: "Decision Trees Assignment"
author: "Abed, Nevin, and Heshan"
date: "2025-04-09"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: true
    toc_depth: 3
    theme: cosmo
    toc_float: true
---


```{r setup, include=FALSE}

if (rstudioapi::isAvailable()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))}

options(width = 200)

# Load libraries ---------------------------------------------------------------
# Create our function "using()" 
using <- \(pkg) {
  # if a package is not installed, install it
  if (!rlang::is_installed(pkg)) {
    install.packages(pkg, repo = "https://cloud.r-project.org")}# load the package
  library(pkg, character.only = TRUE)
}



using("data.table") # The data manipulation king
using("caret") # The ML swiss-knife - http://topepo.github.io/caret/available-models.html
using("plotly") # Beautiful interactive plots
using("ranger") # the fastest and better random forest implementation
using("xgboost") # Extreme Gradient boosting
using("imbalance") # Oversampling
using("ROSE") # Synthetic generation of new data to rebalance
using("VIM")

```


```{r data}
data <- fread("data.csv")

```



```{r}
str(data)
View(data)


```

```{r}
colnames(data)[1:42] <- c(
  "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
  "land", "wrong_fragt", "urgent", "hot", "num_fail_login", "logged_in",
  "nu_comprom", "root_shell", "su_attempted", "num_root", "nu_file_creat",
  "nu_shells", "nu_access_files", "nu_out_cmd", "is_host_login",
  "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
  "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
  "srv_diff_h_rate", "host_count", "host_srv_count", "h_same_sr_rate",
  "h_diff_srv_rate", "h_src_port_rate", "h_srv_d_h_rate", "h_serror_rate",
  "h_sr_serror_rate", "h_rerror_rate", "h_sr_rerror_rate","class"
)

```
```{r}
as.data.frame(table(data$class))
```
```{r}
any(is.na(data)) # 
colSums(is.na(data)) #
sum(complete.cases(data))
# 22469
nrow(data)
# 528603

```


```{r}
# checking for duplicates:
#sum(duplicated(data)) # 128935
#nrow(data) - sum(duplicated(data))# 399668
#remove duplicates
data <- data[!duplicated(data), ]

```
```{r}

#data.frame(names(data))

```

```{r}
data$class <- as.factor(data$class)
data$protocol_type <- as.factor(data$protocol_type)
data$service <- as.factor(data$service)
data$flag <- as.factor(data$flag)




```

Finding no usefullness to samples with empty class, we decided remove them.
```{r}
data[data$class == "", ] |> head(10)
nrow(data[data$class == "", ]) # 36061
data <- data[data$class != '']
```


```{r}

#imputing:
for (j in names(data)) {
  if(is.numeric(data[[j]])) {
    data[is.na(get(j)), (j) := median(data[[j]], na.rm = TRUE)]}} # fucking median imputation values is done now 

nrow(data) # 399668 after removing duplicates
#View(data)
```


Handling missing char/factor values:
```{r}
# Count empty cells in each column
empty_cells <- sapply(data, function(x) sum(x == ""))
View(as.data.frame(empty_cells))
```




Checking for overlaping values:

```{r}
# Count rows with missing values in each combination of factors
missing_protocol <- data$protocol_type == ""
missing_service <- data$service == ""
missing_flag <- data$flag == ""

# Check total rows with at least one missing value
total_missing_rows <- sum(missing_protocol | missing_service | missing_flag)

# Check overlaps
protocol_service_overlap <- sum(missing_protocol & missing_service)
protocol_flag_overlap <- sum(missing_protocol & missing_flag)
service_flag_overlap <- sum(missing_service & missing_flag)
all_three_missing <- sum(missing_protocol & missing_service & missing_flag)
percent_affected <- (total_missing_rows / nrow(data)) * 100
```

Since removing ~25% of our data would be a substantial loss of information, AI recommends a combined approach:

Analyze the patterns of missingness: Check if the missing values are related to specific classes or patterns in the data. This might give insight into whether the missing data is Missing at Random (MAR) or Missing Not at Random (MNAR).
Mode imputation for rows with a single missing value: For rows where only one of the three variables is missing, use mode imputation.
Consider removal for rows with multiple missing values: For rows where two or all three variables are missing (which appears to be a much smaller subset), consider removal if those patterns don't conform to an identifiable subset of your data.




Checking percentages of missing chars in each feature:
```{r}
empty_cells_percentage <- (empty_cells / nrow(data)) * 100
#print(empty_cells_percentage)
#View(as.data.frame(empty_cells_percentage))

empty_cells_df <- data.frame(
  Column = names(empty_cells),
  Empty_Cells = empty_cells,
  Percentage = empty_cells_percentage
)
print(empty_cells_df)


```
```{r}

# Sort by percentage of empty cells
empty_cells_df <- empty_cells_df[order(-empty_cells_df$Percentage), ]
print(empty_cells_df)


```


```{r}
length(unique(data$flag))
length(unique(data$service))
length(unique(data$protocol_type))
```
> length(unique(data$flag))
[1] 13
> length(unique(data$service))
[1] 68
> length(unique(data$protocol_type))
[1] 5


```{r}
# Create a table of unique values for each feature
unique_values <- sapply(data, function(x) length(unique(x)))
unique_values_df <- data.frame(
  Feature = names(unique_values),
  Unique_Values = unique_values)

# Sort by number of unique values
unique_values_df <- unique_values_df[order(-unique_values_df$Unique_Values), ]

# Print the table
print(unique_values_df)

# View in a more readable format
View(unique_values_df)
```



Random Forest Model + Comparing Training vs Testing

```{r, include=FALSE}}

# Split data into training and testing sets
set.seed(42)
train_index <- createDataPartition(data$class, p = 0.7, list = FALSE)
train_data <- data[train_index]
test_data <- data[-train_index]

# Train Random Forest model with ranger
model_rf <- ranger(
  class ~ ., 
  data = train_data,
  importance = 'impurity',  # For feature importance
  num.trees = 100,          # Start with fewer trees for speed
  mtry = floor(sqrt(ncol(data) - 1)), # Default RF formula
  min.node.size = 5,        # Smaller for better precision
  verbose = TRUE            # Track progress
)

# Making predictions
pred_rf <- predict(model_rf, data = test_data)


# NOTE: I GET ERROR IN CONFUSION MATRIX
# DOESNT WORK: Error in `[.default`(data, , pos) : subscript out of bounds
# confusionMatrix(pred_rf$predictions, test_data$class)


#SO WE USE THIS INSTEAD!
# Alternative method to evaluate predictions
table_result <- table(Predicted = pred_rf$predictions, Actual = test_data$class)
print(table_result)

# Calculate accuracy manually
# Predict on training data
train_pred_rf <- predict(model_rf, data = train_data)

# Create confusion matrix for training data
train_table_result <- table(Predicted = train_pred_rf$predictions, Actual = train_data$class)

# Calculate training accuracy manually
train_accuracy <- sum(diag(train_table_result)) / sum(train_table_result)


# Compare training and testing accuracy
print(paste("Training Accuracy:", round(train_accuracy, 4)))
print(paste("Testing Accuracy:", round(accuracy, 4)))

# Get feature importance
importance_df <- data.frame(
  Feature = names(model_rf$variable.importance),
  Importance = model_rf$variable.importance
)
importance_df <- importance_df[order(-importance_df$Importance), ]
head(importance_df, 10)  # Show top 10 features



```
Accuracy on testing 0.9993"




Metric Table

```{r}
# Calculate per-class precision, recall, and F1 score
metrics_by_class <- data.frame(
  Class = colnames(table_result),
  Precision = diag(table_result) / colSums(table_result),
  Recall = diag(table_result) / rowSums(table_result),
  Support = rowSums(table_result)
)

# Add F1 score
metrics_by_class$F1 <- 2 * (metrics_by_class$Precision * metrics_by_class$Recall) / 
                        (metrics_by_class$Precision + metrics_by_class$Recall)

# Replace NaN values with 0
metrics_by_class[is.na(metrics_by_class)] <- 0

# Print the metrics
print(metrics_by_class[order(-metrics_by_class$Support),])

# Calculate macro averages
macro_avg <- data.frame(
  Class = "macro_avg",
  Precision = mean(metrics_by_class$Precision, na.rm = TRUE),
  Recall = mean(metrics_by_class$Recall, na.rm = TRUE),
  Support = sum(metrics_by_class$Support),
  F1 = mean(metrics_by_class$F1, na.rm = TRUE)
)

# Weighted average (weight by support)
weighted_avg <- data.frame(
  Class = "weighted_avg",
  Precision = sum(metrics_by_class$Precision * metrics_by_class$Support, na.rm = TRUE) / 
              sum(metrics_by_class$Support, na.rm = TRUE),
  Recall = sum(metrics_by_class$Recall * metrics_by_class$Support, na.rm = TRUE) / 
           sum(metrics_by_class$Support, na.rm = TRUE),
  Support = sum(metrics_by_class$Support),
  F1 = sum(metrics_by_class$F1 * metrics_by_class$Support, na.rm = TRUE) / 
        sum(metrics_by_class$Support, na.rm = TRUE)
)

# Combine all metrics
all_metrics <- rbind(metrics_by_class, macro_avg, weighted_avg)
print(all_metrics)

# Visualize feature importance
using("ggplot2")
ggplot(importance_df[1:20,], aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Top 20 Features by Importance", 
       x = "Features", 
       y = "Importance Score")

```


k-fold cross validation

```{r}
# K-fold cross-validation
k <- 5
folds <- createFolds(data$class, k = k)
cv_results <- data.frame()

for (i in 1:k) {
  # Split data
  test_indices <- folds[[i]]
  cv_train <- data[-test_indices]
  cv_test <- data[test_indices]
  
  # Train model
  cv_model <- ranger(class ~ ., data = cv_train, num.trees = 100)
  
  # Predict
  cv_pred <- predict(cv_model, data = cv_test)
  cv_acc <- sum(cv_pred$predictions == cv_test$class) / nrow(cv_test)
  
  # Store results
  cv_results <- rbind(cv_results, data.frame(Fold = i, Accuracy = cv_acc))
}

# Calculate mean and standard deviation
mean_acc <- mean(cv_results$Accuracy)
sd_acc <- sd(cv_results$Accuracy)
print(paste("Mean CV Accuracy:", round(mean_acc, 4), "±", round(sd_acc, 4)))

```

We got 0.999 both on training/testing AND on the validation, which is incredibly good

