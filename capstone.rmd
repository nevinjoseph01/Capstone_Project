---
title: "Capstone Project"
author: "Abed, Nevin, and Heshan"
date: "2025-04-09"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: true
    toc_depth: 3
    theme: cosmo
    toc_float: true
---

## Potential research Questions:
+ Create a machine learning model to correctly predict the class of the network data.
+ Use different preprocessing methods to clean, deduplicate and standardize the data.
+ Explore different types of feature selection algorithms, perform a comparative study to find the most effective feature selection algorithm.
+ Test the model with k-fold cross validation and check for any discrepancies.

### Suggestions for Qs improvement:
+ More Specific Prediction Goals: "How effective are different ML algorithms in detecting specific attack types (e.g., DoS vs Probe) compared to normal traffic?"
+ Protocol-Specific Analysis: "How do attack patterns and detection accuracy differ across various network protocols (TCP, UDP, ICMP)?"
+ Feature Importance Analysis: "Which network traffic features are most predictive of specific attack types, and does this vary by protocol?"
+ Imbalance Handling Comparison: "How do different class balancing techniques affect the model's ability to detect rare attack types?"


```{r setup, include=FALSE}

if (rstudioapi::isAvailable()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))}

options(width = 200)

# Load libraries ---------------------------------------------------------------
# Create our function "using()" 
using <- \(pkg) {
  # if a package is not installed, install it
  if (!rlang::is_installed(pkg)) {
    install.packages(pkg, repo = "https://cloud.r-project.org")}# load the package
  library(pkg, character.only = TRUE)
}



using("data.table") # The data manipulation king
using("caret") # The ML swiss-knife - http://topepo.github.io/caret/available-models.html
using("plotly") # Beautiful interactive plots
using("ranger") # the fastest and better random forest implementation
using("xgboost") # Extreme Gradient boosting
using("imbalance") # Oversampling
using("ROSE") # Synthetic generation of new data to rebalance
using("VIM")
using("smotefamily")

```


```{r data}
data <- fread("data.csv")

```



```{r}
str(data)
#View(data)


```

### Renaming columns according to the dataset description

```{r}
colnames(data)[1:42] <- c(
  "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
  "land", "wrong_fragt", "urgent", "hot", "num_fail_login", "logged_in",
  "nu_comprom", "root_shell", "su_attempted", "num_root", "nu_file_creat",
  "nu_shells", "nu_access_files", "nu_out_cmd", "is_host_login",
  "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
  "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
  "srv_diff_h_rate", "host_count", "host_srv_count", "h_same_sr_rate",
  "h_diff_srv_rate", "h_src_port_rate", "h_srv_d_h_rate", "h_serror_rate",
  "h_sr_serror_rate", "h_rerror_rate", "h_sr_rerror_rate","class"
)

```
### Checking for class imbalance
```{r}
as.data.frame(table(data$class))
```
We can obviously identify multiple classes with relatively significantly low frequency.

```{r}
# Remove all classes with frequency <= 54
class_counts <- table(data$class)
rare_classes <- names(class_counts[class_counts <= 54])
data <- data[!data$class %in% rare_classes]
```


```{r}
cat("is there any missing values? ", any(is.na(data))) # 
```
Features with missing values:
```{r}
#colSums(is.na(data)) #
library(knitr)

# Get the NA counts as a named vector
na_vector <- colSums(is.na(data))

# Convert to a data frame for kable()
na_df <- data.frame(Variable = names(na_vector), NA_Count = as.numeric(na_vector))

# Optional: Sort by NA_Count
na_df <- na_df[order(-na_df$NA_Count), ]

# Use kable to print a nice table
kable(na_df, caption = "Missing Values per Column")
```

```{r}
cat("Number of complete cases:", sum(complete.cases(data)))
# 22469
#cat("Number of rows:", nrow(data))
# 528603

```

### Checking for duplicates:
```{r}

#sum(duplicated(data)) # 128935
#nrow(data) - sum(duplicated(data))# 399668
#remove duplicates
data <- data[!duplicated(data), ]

```
Number of duplicated samples: 128935
Number of samples after removing duplicates: 399668

```{r}
data$class <- as.factor(data$class)
data$protocol_type <- as.factor(data$protocol_type)
data$service <- as.factor(data$service)
data$flag <- as.factor(data$flag)
```

Finding no usefullness to samples with empty class, we decided remove them.
```{r}
#data[data$class == "", ] |> head(10)
cat("Number of samples with empty class:", nrow(data[data$class == "", ]))
data <- data[data$class != '']
```


```{r}
#imputing:
for (j in names(data)) {
  if(is.numeric(data[[j]])) {
    data[is.na(get(j)), (j) := median(data[[j]], na.rm = TRUE)]}} # median imputation values is done now 

cat("Number of rows after imputing:", nrow(data))
#View(data)
```

Removing outliers:
Can't remove it right now because the target detection is not binary, but about 26 classes.
The following chunk is disabled for "run above" button
```{r, include=FALSE}
# removing outliers:
training_without_outliers <- copy(data)
numeric_cols <- names(data)[sapply(data, is.numeric)]
for(i in numeric_cols) { 
  for(j in unique(data$class)) {
    values <- data[class == j, get(i)]
    Q1 <- quantile(values, 0.25)
    Q3 <- quantile(values, 0.75)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    training_without_outliers <- training_without_outliers[!(
      class == j & (get(i) < lower_bound | get(i) > upper_bound))]}}

nrow(training_without_outliers)/nrow(data)
# 0.4350536
```


Handling missing char/factor values:
```{r}
# Count empty cells in each column
empty_cells <- sapply(data, function(x) sum(x == ""))
#View(as.data.frame(empty_cells))
```




Checking for overlaping values:

```{r}
# Count rows with missing values in each combination of factors
missing_protocol <- data$protocol_type == ""
missing_service <- data$service == ""
missing_flag <- data$flag == ""

# Check total rows with at least one missing value
total_missing_rows <- sum(missing_protocol | missing_service | missing_flag)

# Check overlaps
protocol_service_overlap <- sum(missing_protocol & missing_service)
protocol_flag_overlap <- sum(missing_protocol & missing_flag)
service_flag_overlap <- sum(missing_service & missing_flag)
all_three_missing <- sum(missing_protocol & missing_service & missing_flag)
percent_affected <- (total_missing_rows / nrow(data)) * 100
```
There is ~9% missing data from each 'protocol_type', 'service', and 'flag' columns.


Since removing ~25% of our data would be a substantial loss of information, AI recommends a combined approach:

Analyze the patterns of missingness: Check if the missing values are related to specific classes or patterns in the data. This might give insight into whether the missing data is Missing at Random (MAR) or Missing Not at Random (MNAR).
Mode imputation for rows with a single missing value: For rows where only one of the three variables is missing, use mode imputation.
Consider removal for rows with multiple missing values: For rows where two or all three variables are missing (which appears to be a much smaller subset), consider removal if those patterns don't conform to an identifiable subset of your data.




Checking percentages of missing chars in each feature:
```{r}
empty_cells_percentage <- (empty_cells / nrow(data)) * 100
#print(empty_cells_percentage)
#View(as.data.frame(empty_cells_percentage))

empty_cells_df <- data.frame(
  Column = names(empty_cells),
  Empty_Cells = empty_cells,
  Percentage = empty_cells_percentage
)
# Sort by percentage of empty cells
empty_cells_df <- empty_cells_df[order(-empty_cells_df$Percentage), ]
#print(empty_cells_df)



```

```{r}
cat("Number of unique flag values:", length(unique(data$flag)))
cat("Number of unique service values:", length(unique(data$service)))
cat("Number of unique protocol_type values:", length(unique(data$protocol_type)))
```

# Number of unique values for each feature:

```{r}
# Create a table of unique values for each feature
#unique_values <- sapply(data, function(x) length(unique(x)))
#unique_values_df <- data.frame(
#  Feature = names(unique_values),
#  Unique_Values = unique_values)

## Sort by number of unique values
#unique_values_df <- unique_values_df[order(-unique_values_df$Unique_Values), ]


library(knitr) # Make sure this library is loaded

unique_values <- sapply(data, function(x) length(unique(x)))
unique_values_df <- data.frame(
  Feature = names(unique_values),
  Unique_Values = unique_values
)

# Sort by number of unique values
unique_values_df <- unique_values_df[order(-unique_values_df$Unique_Values), ]

# Use kable to print a nice table, suppressing row names
kable(unique_values_df,
      caption = "Number of Unique Values for Each Feature",
      align = c("l", "c"), # Left-align Feature, Center-align Unique_Values
      row.names = FALSE     # <--- This is the key!
)
```
This shows us that some categorical features have limited unique values which could help us subset the data.

```{r}
#print(unique_values_df)
# View in a more readable format
#View(unique_values_df)
```


## Normal connections
```{r}
normal_connections <- data[data$class == "b'normal.'", ]

cat("Number of normal connections:", nrow(normal_connections))
```
There seems to be ~9.5K non-malicious connections in the dataset. This could be considered for further sub-setting.


## Handling Class Imbalance

#### Before balancing:
```{r}
before_plot <- ggplot(as.data.frame(table(data$class)), aes(x = reorder(Var1, -Freq), y = Freq)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Class Distribution Before Balancing",
       x = "Class", y = "Frequency")
```


```{r}
# Set a balanced target for each class
target_samples <- 2000  # We'll aim for 2000 samples per class

# Create a balanced dataset using both undersampling and SMOTE
balanced_data <- data.table()       

# First, undersample the majority classes
for (cls in unique(data$class[data$class != ""])) {
  class_data <- data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    # Undersample the majority classes
    set.seed(123)  # For reproducibility
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data <- rbind(balanced_data, balanced_class_data)
  } else {
    # Keep all samples from minority classes
    balanced_data <- rbind(balanced_data, class_data)
  }
}
```

### Now handling the minority classes using SMOTE
```{r}
# First convert to data.frame (SMOTE functions expect data.frames)
balanced_df <- as.data.frame(balanced_data)

# We need to convert factor variables to numeric for SMOTE
# Store original factor columns and their levels
factor_cols <- names(balanced_df)[sapply(balanced_df, is.factor)]
factor_levels <- lapply(balanced_df[, factor_cols, drop = FALSE], levels)

# Convert factors to numeric
for (col in factor_cols) {
  balanced_df[[col]] <- as.numeric(balanced_df[[col]])
}

# Apply SMOTE to oversample minority classes
set.seed(123)

# Create a binary classifier for each minority class
for (cls in unique(balanced_data$class[balanced_data$class != ""])) {
  class_count <- sum(balanced_data$class == cls)
  
  if (class_count < target_samples && class_count > 0) {
    # Create a binary classification problem for this class
    temp_df <- balanced_df
    temp_df$target <- ifelse(temp_df$class == as.numeric(factor(cls)), 1, 0)
    
    # Count samples in this class
    n_minority <- sum(temp_df$target == 1)
    
    if (n_minority >= 5) {  # SMOTE needs at least a few samples to work with
      # Determine how many synthetic samples to create
      n_synthetic <- target_samples - n_minority
      
      # Apply SMOTE with dup_size parameter (ratio of synthetic to original samples)
      dup_size <- ceiling(n_synthetic / n_minority)
      
      # Select only the rows for this class to apply SMOTE
      minority_data <- temp_df[temp_df$target == 1, ]
      
      # Only proceed if we have enough samples
      if (nrow(minority_data) >= 5) {
        # Create synthetic samples using SMOTE
        smote_result <- try(SMOTE(
          X = minority_data[, !names(minority_data) %in% c("target", "class")],
          target = minority_data$target,
          K = min(5, nrow(minority_data) - 1),  # Use fewer neighbors if needed
          dup_size = dup_size
        ), silent = TRUE)
        
        if (!inherits(smote_result, "try-error")) {
          # Extract the synthetic samples
          synthetic_samples <- smote_result$syn_data
          
          # Add back the class column
          synthetic_samples$class <- as.numeric(factor(cls))
          
          # Remove the target column if it exists
          if ("target" %in% names(synthetic_samples)) {
            synthetic_samples$target <- NULL
          }
          
          # Limit the number of synthetic samples to what we need
          if (nrow(synthetic_samples) > n_synthetic) {
            synthetic_samples <- synthetic_samples[1:n_synthetic, ]
          }
          
          # Add the synthetic samples to our balanced dataframe
          if (nrow(synthetic_samples) > 0) {
            balanced_df <- rbind(balanced_df[balanced_df$class != as.numeric(factor(cls)), ],
                                rbind(balanced_df[balanced_df$class == as.numeric(factor(cls)), ],
                                     synthetic_samples))
          }
        }
      }
    }
  }
}

# Remove the target column if it exists
if ("target" %in% names(balanced_df)) {
  balanced_df$target <- NULL
}

# Convert back to factors using the original levels
for (col in factor_cols) {
  balanced_df[[col]] <- factor(balanced_df[[col]], levels = 1:length(factor_levels[[col]]), 
                              labels = factor_levels[[col]])
}
```


```{r}
# Convert back to data.table
balanced_data <- as.data.table(balanced_df)

# remove empty class "":
balanced_data <- balanced_data[!(class == "")]
balanced_data <- balanced_data[!(class == " ")]
balanced_data <- balanced_data[!(class == "V1")]


# Check the new class distribution
print("Balanced class distribution:")
balanced_class_table <- as.data.frame(table(balanced_data$class))
print(balanced_class_table)

# Verify the total number of samples
cat("Original dataset size:", nrow(data), "\n")
cat("Balanced dataset size:", nrow(balanced_data), "\n")

# Alternative approach: Using simple random oversampling for smaller classes
# This is a simpler approach that might work better in your case

# Create a new data.table for the simpler approach
balanced_data_simple <- data.table()

set.seed(123)  # For reproducibility

for (cls in unique(data$class[data$class != ""])) {
  class_data <- data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    # Undersample the majority classes
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples > 0 && n_samples < target_samples) {
    # Random oversampling with replacement for minority classes
    oversampled_indices <- sample(1:n_samples, target_samples, replace = TRUE)
    balanced_class_data <- class_data[oversampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples == target_samples) {
    # Keep as is if already at target
    balanced_data_simple <- rbind(balanced_data_simple, class_data)
  }
}

# Check the new class distribution using the simple approach
print("Balanced class distribution (simple approach):")
balanced_class_table_simple <- as.data.frame(table(balanced_data_simple$class))
print(balanced_class_table_simple)

# Verify the total number of samples
cat("Balanced dataset size (simple approach):", nrow(balanced_data_simple), "\n")

# Use the simpler balanced dataset for further modeling
balanced_data <- balanced_data_simple
```


```{r}
"
Before Balancing:

              Var1   Freq
1                       0
2         b'back.'   2148
3      b'ipsweep.'   1221
4      b'neptune.' 104990
5         b'nmap.'    232
6       b'normal.'  95611
7          b'pod.'    255
8    b'portsweep.'    989
9        b'satan.'   1536
10       b'smurf.' 154486
11    b'teardrop.'    966
12 b'warezclient.'    995



After Balancing:

              Var1 Freq
1                     0
2         b'back.' 2000
3      b'ipsweep.' 2000
4      b'neptune.' 2000
5         b'nmap.' 2000
6       b'normal.' 2000
7          b'pod.' 2000
8    b'portsweep.' 2000
9        b'satan.' 2000
10       b'smurf.' 2000
11    b'teardrop.' 2000
12 b'warezclient.' 2000"
```



```{r}
# After balancing
after_plot <- ggplot(balanced_class_table_simple, aes(x = reorder(Var1, -Freq), y = Freq)) +
  geom_bar(stat = "identity", fill = "forestgreen") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Class Distribution After Balancing",
       x = "Class", y = "Frequency")

before_plot
after_plot
```




## Preparing splits + Balancing data 

```{r}

# Step 1: Split BEFORE balancing
# ------------------------------
set.seed(42)
train_index <- createDataPartition(data$class, p = 0.7, list = FALSE)
train_data <- data[train_index]
test_data <- data[-train_index]

# Adding explicit empty class removal after splitting
train_data <- train_data[train_data$class != '']
test_data <- test_data[test_data$class != '']

# ------------------------------
# Step 2: Apply your balancing code to train_data
# ------------------------------
# IMPORTANT: We only balance the training data, not the test data
# This ensures our model evaluation is performed on a realistic, unbalanced distribution

# Set a balanced target for each class
target_samples <- 2000  # We'll aim for 2000 samples per class

# Create a balanced dataset using both undersampling and oversampling (simpler path)
balanced_data_simple <- data.table()

# Document preprocessing steps for consistency
cat("\nPreprocessing consistency check:\n")
cat("- Train data rows before balancing:", nrow(train_data), "\n")
cat("- Test data rows:", nrow(test_data), "\n")

# Iterate through each class and balance
cat("\nBalancing classes in training data only:\n")
for (cls in unique(train_data$class[train_data$class != ""])) {
  class_data <- train_data[class == cls]
  class_size <- nrow(class_data)
  cat("- Class '", cls, "': ", class_size, " samples", sep="")
  
  if (class_size > target_samples) {
    # Undersample for large classes
    set.seed(42)  # Ensure reproducibility
    balanced_class_data <- class_data[sample(.N, target_samples)]
    cat(" → undersampled to ", target_samples, " samples\n", sep="")
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (class_size < target_samples) {
    # Oversample for small classes
    set.seed(42)  # Ensure reproducibility
    balanced_class_data <- class_data[sample(.N, target_samples, replace = TRUE)]
    cat(" → oversampled to ", target_samples, " samples\n", sep="")
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else {
    # Class already at target size
    cat(" → already at target size\n")
    balanced_data_simple <- rbind(balanced_data_simple, class_data)
  }
}

# Use this as training data
balanced_data <- balanced_data_simple

# ------------------------------
# Step 3: Check class distributions
# ------------------------------
cat("Balanced training set class distribution:\n")
print(as.data.frame(table(balanced_data$class)))

cat("Unbalanced test set class distribution:\n")
print(as.data.frame(table(test_data$class))) 
```

### Protocol distribution
```{r}
# Check the current protocol distribution
cat("Protocol distribution before fixing unnamed labels:\n")
print(table(balanced_data$protocol_type))

# Check for unnamed or empty protocol types
cat("\nChecking for unnamed protocol types:\n")
cat("- Empty strings:", sum(balanced_data$protocol_type == ""), "\n")
cat("- NA values:", sum(is.na(balanced_data$protocol_type)), "\n")
cat("- Blank spaces:", sum(balanced_data$protocol_type == " "), "\n")

# Get the levels of the factor to see if there's an unnamed level
proto_levels <- levels(balanced_data$protocol_type)
cat("\nProtocol type levels:\n")
print(proto_levels)
```

### Replacing unnamed protocol_type with b'unknown':
```{r, warning=F}
# Convert to character first to handle factor levels properly
balanced_data$protocol_type <- as.character(balanced_data$protocol_type)

# Replace NA values
balanced_data$protocol_type[is.na(balanced_data$protocol_type)] <- "b'unknown'"

# Replace empty strings
balanced_data$protocol_type[balanced_data$protocol_type == ""] <- "b'unknown'"

# Replace unnamed level (if the first level is unnamed)
if (length(proto_levels) > 0 && proto_levels[1] == "") {
  balanced_data$protocol_type[balanced_data$protocol_type == proto_levels[1]] <- "b'unknown'"
}

# Convert back to factor
balanced_data$protocol_type <- as.factor(balanced_data$protocol_type)

# Check the updated distribution
cat("Protocol distribution after fixing unnamed labels:\n")
print(table(balanced_data$protocol_type))
```



## Feature importance
```{r, warning=FALSE}
# Create a copy of the balanced data
feature_selection_data <- copy(balanced_data)

# Add a random feature as a baseline for importance comparison
set.seed(123)
feature_selection_data[, random := runif(nrow(feature_selection_data), 1, 100)]

# Train a quick random forest to get feature importance
fs_model <- ranger(
  formula = class ~ ., 
  data = feature_selection_data,
  importance = 'impurity',
  num.trees = 100,  # Using fewer trees for quicker analysis
  mtry = floor(sqrt(ncol(feature_selection_data) - 1)),
  verbose = TRUE
)

# Extract feature importance
imp <- importance(fs_model)
imp_df <- data.frame(Feature = names(imp), Importance = imp)

# Sort by importance
imp_df <- imp_df[order(-imp_df$Importance), ]

# Create interactive bar plot of feature importance
importance_plot <- plot_ly(data = imp_df, 
                          x = ~reorder(Feature, Importance), 
                          y = ~Importance, 
                          type = "bar",
                          marker = list(color = "steelblue")) %>%
  layout(title = "Feature Importance for Network Intrusion Detection",
         xaxis = list(title = "Features", categoryorder = "total descending"),
         yaxis = list(title = "Importance Score"))

# Display the plot
importance_plot
```

## Implementing feature selection based on importance
```{r}
## Feature Selection Implementation ----

# Get feature names in order of importance (already in imp_df)



# Select features with importance above certain threshold
# From the plot, we can see importance drops significantly after around 400-500
importance_threshold <- 400
selected_features_threshold <- as.character(imp_df$Feature[imp_df$Importance > importance_threshold])
cat("\nFeatures with importance > ", importance_threshold, ":\n")
cat("Selected Features:\n",selected_features_threshold)


selected_features <- selected_features_threshold

# Create a new dataset with only selected features plus the class column
print(paste("Creating new dataset with", length(selected_features), "selected features"))
balanced_data_selected <- balanced_data[, c(selected_features, "class"), with = FALSE]

# Verify the dimensions
cat("Original dataset dimensions:", dim(balanced_data)[1], "rows and", dim(balanced_data)[2], "columns\n")
cat("Selected dataset dimensions:", dim(balanced_data_selected)[1], "rows and", dim(balanced_data_selected)[2], "columns\n")

# Also prepare test data with same features for consistent evaluation
test_data_selected <- test_data[, c(selected_features, "class"), with = FALSE]

# Update our dataset references to use the selected features
# Comment this line if you want to keep using all features
balanced_data <- balanced_data_selected
test_data <- test_data_selected


```


Based on feature selection, we keep the following features and train our model accordingly. We decided to select the treshold for importance at 400 as seen in the plot.
 "src_bytes"       "wrong_fragt"     "srv_count"       "count"           "service"         "h_srv_d_h_rate"  "same_srv_rate"   "h_src_port_rate" "logged_in"       "host_srv_count"
"rerror_rate"     "dst_bytes"       "protocol_type"   "nu_comprom"      "diff_srv_rate"   "host_count"      "flag"


## Random Forest + Evaluation

```{r}
# Train the Random Forest
# ------------------------------
print("Training Random Forest model...")
model_rf <- ranger(
  formula = class ~ ., 
  data = balanced_data,
  importance = 'impurity',
  num.trees = 500,
  mtry = floor(sqrt(ncol(balanced_data) - 1)),
  min.node.size = 5,
  verbose = TRUE
)

# ------------------------------
# Evaluate on test set
# ------------------------------
# Make sure both datasets have the same factor levels before prediction
# This ensures prediction classes match the training data classes
cat("Checking for empty classes in test data before prediction:\n")
cat("Any empty class values?", any(test_data$class == ""), "\n")

# Ensure class is a factor with the same levels in both datasets
all_classes <- unique(c(levels(balanced_data$class), levels(test_data$class)))
all_classes <- all_classes[all_classes != ""]

# Convert to factor with consistent levels
balanced_data$class <- factor(balanced_data$class, levels = all_classes)
test_data$class <- factor(test_data$class, levels = all_classes)

# Verify factor levels before prediction
cat("\nTraining data class levels:\n")
print(levels(balanced_data$class))
cat("\nTest data class levels:\n")
print(levels(test_data$class))

# Now make predictions with consistent class factors
pred_rf <- predict(model_rf, data = test_data)
predictions <- pred_rf$predictions

# Ensure predictions are factors with the same levels as the test data
predictions <- factor(predictions, levels = levels(test_data$class))

# Create confusion matrix with proper factor levels
conf_matrix <- table(Predicted = predictions, Actual = test_data$class)
print("Confusion Matrix:")

# Convert to data frame for kable, ensuring no empty classes are included
conf_matrix_df <- as.data.frame.matrix(conf_matrix)

# Print using kable
kable(conf_matrix_df,
      caption = "Confusion Matrix",
      align = "c", # Center-align columns for better readability
      row.names = TRUE # Ensure row names (your 'Predicted' categories) are displayed
      )

accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Overall Accuracy:", round(accuracy, 4)))
```

### Per-class metrics

```{r}
# ------------------------------
# Per-class metrics
# ------------------------------
calculate_class_metrics <- function(conf_mat) {
  n_classes <- nrow(conf_mat)
  class_names <- rownames(conf_mat)
  
  metrics <- data.frame(
    Class = class_names,
    Precision = numeric(n_classes),
    Recall = numeric(n_classes),
    F1_Score = numeric(n_classes),
    Support = numeric(n_classes)
  )
  
  for (i in 1:n_classes) {
    tp <- conf_mat[i, i]
    fp <- sum(conf_mat[, i]) - tp
    fn <- sum(conf_mat[i, ]) - tp
    precision <- if(tp + fp > 0) tp / (tp + fp) else 0
    recall <- if(tp + fn > 0) tp / (tp + fn) else 0
    f1 <- if(precision + recall > 0) 2 * precision * recall / (precision + recall) else 0
    
    metrics$Precision[i] <- precision
    metrics$Recall[i] <- recall
    metrics$F1_Score[i] <- f1
    metrics$Support[i] <- sum(conf_mat[i, ])
  }
  
  return(metrics)
}

class_metrics <- calculate_class_metrics(conf_matrix)
print("Per-class metrics:")
print(class_metrics)

macro_avg <- colMeans(class_metrics[, c("Precision", "Recall", "F1_Score")])
print("Macro Average metrics:")
print(macro_avg)
```

Our Random Forest model achieves exceptional performance on the network intrusion detection task with 99.6% overall accuracy, demonstrating robust classification capabilities across various attack types. The macro-averaged metrics (Precision: 0.907, Recall: 0.842, F1-Score: 0.868) confirm strong generalization ability despite class imbalance. Several attack classes show near-perfect detection, including neptune (F1: 0.999), smurf (F1: 0.9999), normal traffic (F1: 0.993), teardrop (F1: 0.997), and pod (F1: 0.981). Moderate performance is observed for back (F1: 0.983), satan (F1: 0.968), and portsweep (F1: 0.939) attacks. The most significant challenge appears with warezclient attacks, which despite perfect precision suffer from lower recall (0.535), with many instances misclassified as normal traffic. Similarly, ipsweep (Recall: 0.879) and nmap (Precision/Recall: 0.928) show room for improvement. The feature selection approach has clearly identified the most informative network traffic characteristics, enabling the model to effectively distinguish between normal traffic and various attack types, though future refinements should focus on improving detection of warezclient attacks to further enhance the system's security capabilities.

## Protocol-Based Analysis and Visualization

Given the large magnitude of our dataset, we'll split it by protocol_type (TCP, UDP, ICMP) and create visualizations for each subset to gain deeper insights into protocol-specific patterns.

```{r protocol_split}
# Make sure we have the original data available
if (!exists("data") || nrow(data) == 0) {
  # If data is not available, load it
  if (file.exists("data.csv")) {
    data <- fread("data.csv")
    # Ensure proper column names and factor conversion
    if (ncol(data) >= 42) {
      colnames(data)[1:42] <- c(
        "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
        "land", "wrong_fragt", "urgent", "hot", "num_fail_login", "logged_in",
        "nu_comprom", "root_shell", "su_attempted", "num_root", "nu_file_creat",
        "nu_shells", "nu_access_files", "nu_out_cmd", "is_host_login",
        "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
        "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
        "srv_diff_h_rate", "host_count", "host_srv_count", "h_same_sr_rate",
        "h_diff_srv_rate", "h_src_port_rate", "h_srv_d_h_rate", "h_serror_rate",
        "h_sr_serror_rate", "h_rerror_rate", "h_sr_rerror_rate","class"
      )
      data$class <- as.factor(data$class)
      data$protocol_type <- as.factor(data$protocol_type)
      data$service <- as.factor(data$service)
      data$flag <- as.factor(data$flag)
    }
  } else {
    # Create a sample dataset for testing if data.csv is not available
    cat("Data file not found. Creating a sample dataset for testing.\n")
    set.seed(123)
    data <- data.frame(
      duration = runif(1000, 0, 100),
      protocol_type = sample(c("tcp", "udp", "icmp"), 1000, replace = TRUE),
      service = sample(c("http", "ftp", "smtp", "dns"), 1000, replace = TRUE),
      flag = sample(c("SF", "REJ", "S0"), 1000, replace = TRUE),
      src_bytes = rpois(1000, 1000),
      dst_bytes = rpois(1000, 500),
      class = sample(c("normal", "neptune", "smurf"), 1000, replace = TRUE)
    )
    # Convert to factors
    data$protocol_type <- as.factor(data$protocol_type)
    data$service <- as.factor(data$service)
    data$flag <- as.factor(data$flag)
    data$class <- as.factor(data$class)
  }
}

# Get unique protocols with error handling
protocols <- unique(as.character(data$protocol_type))
protocols <- protocols[!is.na(protocols) & protocols != ""]

cat("Available protocols:", paste(protocols, collapse = ", "), "\n")

# Create a list to store the protocol-specific datasets
protocol_data <- list()

# Split the data with error handling
for (protocol in protocols) {
  subset_data <- data[data$protocol_type == protocol, ]
  if (nrow(subset_data) > 0) {
    protocol_data[[protocol]] <- subset_data
    cat("Protocol", protocol, "has", nrow(protocol_data[[protocol]]), "samples\n")
  } else {
    cat("Protocol", protocol, "has no samples, skipping...\n")
  }
}
```

```{r}
# Load required libraries
library(ggplot2)
library(plotly)
library(dplyr)
library(reshape2)

# Create the data from your R output
tcp_data <- data.frame(
  class = c("back", "ipsweep", "neptune", "nmap", "normal", "portsweep", "satan", "warezclient"),
  count = c(1982, 84, 96467, 93, 69546, 905, 1281, 916),
  protocol = "TCP"
)

udp_data <- data.frame(
  class = c("nmap", "normal", "satan", "teardrop"),
  count = c(20, 17272, 149, 882),
  protocol = "UDP"
)

icmp_data <- data.frame(
  class = c("ipsweep", "nmap", "normal", "pod", "portsweep", "satan", "smurf"),
  count = c(1039, 96, 1156, 230, 1, 3, 138417),
  protocol = "ICMP"
)


all_data <- rbind(tcp_data, udp_data, icmp_data)
all_classes <- unique(all_data$class)
complete_data <- expand.grid(class = all_classes, protocol = c("TCP", "UDP", "ICMP"))
complete_data <- merge(complete_data, all_data, by = c("class", "protocol"), all.x = TRUE)

# ============================================================
# OPTION 3: Side-by-side overlapping bars (if you prefer this style)
# ============================================================

# Plot with overlapping bars (using position = "identity")
overlapping_plot <- ggplot(complete_data, aes(x = class, y = count, fill = protocol)) +
  geom_bar(stat = "identity", position = "identity", alpha = 0.6) +
  scale_fill_manual(values = c("TCP" = "#3498db", "UDP" = "#2ecc71", "ICMP" = "#f39c12")) +
  labs(title = "Network Protocol Attack Class Distribution - Overlapping Bars",
       x = "Attack Class",
       y = "Count",
       fill = "Protocol") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_y_continuous(labels = scales::comma)

overlapping_interactive <- ggplotly(overlapping_plot, tooltip = c("fill", "x", "y"))
overlapping_interactive

# ============================================================
# OPTION 4: Log scale version (useful given the large range in values)
# ============================================================

# Log scale version for better visualization of smaller values
log_plot <- ggplot(complete_data[complete_data$count > 0,], aes(x = class, y = count, fill = protocol)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  scale_fill_manual(values = c("TCP" = "#3498db", "UDP" = "#2ecc71", "ICMP" = "#f39c12")) +
  scale_y_log10(labels = scales::comma) +
  labs(title = "Network Protocol Attack Class Distribution - Log Scale",
       x = "Attack Class",
       y = "Count (Log Scale)",
       fill = "Protocol") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

log_interactive <- ggplotly(log_plot, tooltip = c("fill", "x", "y"))
log_interactive

```
### Key Observations:
##### TCP Protocol:
Dominated by Neptune attacks (96,467) and normal traffic (69,546)

##### UDP Protocol:
Mostly normal traffic (17,272) with some teardrop attacks (882)

##### ICMP Protocol:
Heavily dominated by Smurf attacks (138,417)


```{r}
# Summary of class distribution by protocol
for (protocol in names(protocol_data)) {
  if (nrow(protocol_data[[protocol]]) > 0) {
    cat("\nClass distribution for protocol", protocol, ":\n")
    class_table <- table(protocol_data[[protocol]]$class)
    class_df <- as.data.frame(class_table)
    print(class_df)
  }
}
```

Now we'll create interactive visualizations for each protocol subset to better understand the characteristics and patterns.

### TCP Protocol Analysis

```{r tcp_visualizations, fig.width=10, fig.height=6, warning=FALSE, message=FALSE}
# Check if TCP protocol exists and has data
if ("tcp" %in% names(protocol_data) && nrow(protocol_data[['tcp']]) > 0) {
  tcp_data <- protocol_data[['tcp']]
  
  # Create a more informative title
  cat("\n### TCP Protocol Analysis", "\n")
  cat("Number of TCP connections:", nrow(tcp_data), "\n")
  cat("Number of attack types in TCP:", length(unique(tcp_data$class)), "\n\n")
  
  # 1. Attack Distribution in TCP - More informative with percentages
  tcp_class_counts <- table(tcp_data$class)
  tcp_class_df <- data.frame(
    Class = names(tcp_class_counts),
    Count = as.numeric(tcp_class_counts),
    Percentage = round(100 * as.numeric(tcp_class_counts) / sum(tcp_class_counts), 2)
  )
  tcp_class_df <- tcp_class_df[order(tcp_class_df$Count, decreasing = TRUE),]
  
  # Print the table for reference
  print(tcp_class_df)
  
  # Create a more informative bar plot
  par(mar = c(10, 4, 4, 2) + 0.1)  # Increase bottom margin for labels
  barplot(tcp_class_df$Count, 
          names.arg = paste0(tcp_class_df$Class, "\n(", tcp_class_df$Percentage, "%)"),
          col = rainbow(nrow(tcp_class_df)),
          main = "Attack Type Distribution in TCP Protocol",
          ylab = "Count",
          las = 2,  # Rotate labels
          cex.names = 0.8)  # Smaller text for labels
  
  # Add a log scale version for better visibility of smaller classes
  barplot(log10(tcp_class_df$Count + 1), 
          names.arg = paste0(tcp_class_df$Class, "\n(", tcp_class_df$Percentage, "%)"),
          col = rainbow(nrow(tcp_class_df)),
          main = "Attack Type Distribution in TCP Protocol (Log Scale)",
          ylab = "Log10(Count + 1)",
          las = 2,
          cex.names = 0.8)
  
  # 2. Service Distribution by Attack Type - More informative
  # Get top services
  top_services <- names(sort(table(tcp_data$service), decreasing = TRUE)[1:min(10, length(unique(tcp_data$service)))])
  
  # Create a contingency table of service vs class
  service_class_table <- table(tcp_data$service, tcp_data$class)
  
  # Filter to top services
  top_service_class <- service_class_table[top_services,]
  
  # Convert to data frame for plotting
  service_class_df <- as.data.frame.matrix(top_service_class)
  
  # Print the table for reference
  cat("\nTop Services by Attack Type:\n")
  print(service_class_df)
  
  # Create a heatmap for better visualization
  # Convert to long format for plotting
  service_class_long <- data.frame(
    Service = rep(rownames(service_class_df), ncol(service_class_df)),
    AttackType = rep(colnames(service_class_df), each = nrow(service_class_df)),
    Count = as.vector(as.matrix(service_class_df))
  )
  
  # Plot as a heatmap
  if (nrow(service_class_long) > 0) {
    # Use a better color palette
    heatmap_colors <- colorRampPalette(c("white", "yellow", "orange", "red"))(100)
    
    # Create the heatmap
    heatmap(as.matrix(service_class_df), 
            col = heatmap_colors,
            main = "Service vs Attack Type Heatmap (TCP)",
            xlab = "Attack Type", 
            ylab = "Service",
            cexRow = 0.8,  # Adjust row label size
            cexCol = 0.8,  # Adjust column label size
            margins = c(10, 10)  # Adjust margins
    )
  }
  
  # 3. Feature importance for distinguishing attack types in TCP
  # Select numeric columns for analysis
  numeric_cols <- sapply(tcp_data, is.numeric)
  if (sum(numeric_cols) > 2) {
    tcp_numeric <- tcp_data[, numeric_cols]
    
    # Calculate mean values by class
    tcp_means_by_class <- aggregate(tcp_numeric, by = list(Class = tcp_data$class), mean, na.rm = TRUE)
    
    # Calculate overall means
    overall_means <- colMeans(tcp_numeric, na.rm = TRUE)
    
    # Calculate how much each feature varies by class (feature importance proxy)
    feature_variation <- apply(tcp_means_by_class[, -1], 2, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE))
    
    # Select top varying features
    top_features <- names(sort(feature_variation, decreasing = TRUE)[1:min(10, length(feature_variation))])
    
    # Print the top features
    cat("\nTop Discriminating Features for TCP Attacks:\n")
    feature_importance_df <- data.frame(
      Feature = names(feature_variation),
      Variation = feature_variation
    )
    feature_importance_df <- feature_importance_df[order(feature_importance_df$Variation, decreasing = TRUE),]
    print(head(feature_importance_df, 10))
    
    # Plot the top features
    par(mar = c(10, 4, 4, 2) + 0.1)
    barplot(feature_importance_df$Variation[1:min(10, nrow(feature_importance_df))],
            names.arg = feature_importance_df$Feature[1:min(10, nrow(feature_importance_df))],
            col = "skyblue",
            main = "Top Discriminating Features for TCP Attacks",
            ylab = "Coefficient of Variation",
            las = 2)
    
    # Create boxplots for the top 3 features by class
    if (length(top_features) >= 3) {
      for (i in 1:min(3, length(top_features))) {
        feature <- top_features[i]
        boxplot(tcp_data[[feature]] ~ tcp_data$class, 
                main = paste("Distribution of", feature, "by Attack Type (TCP)"),
                xlab = "Attack Type",
                ylab = feature,
                col = rainbow(length(unique(tcp_data$class))),
                las = 2,
                cex.axis = 0.7)
      }
    }
  }
  
  # 4. Duration analysis by attack type
  # Create boxplots of duration by attack type
  boxplot(log10(tcp_data$duration + 1) ~ tcp_data$class,
          main = "Connection Duration by Attack Type (TCP) - Log Scale",
          xlab = "Attack Type",
          ylab = "Log10(Duration + 1)",
          col = rainbow(length(unique(tcp_data$class))),
          las = 2,
          cex.axis = 0.7)
  
  # 5. Error Rate Analysis - More informative scatterplot
  # Create a more meaningful scatterplot with density contours
  if (all(c("serror_rate", "rerror_rate") %in% colnames(tcp_data))) {
    # Sample data for better visualization
    set.seed(123)
    sample_size <- min(5000, nrow(tcp_data))
    if (sample_size > 0) {
      sampled_tcp <- tcp_data[sample(1:nrow(tcp_data), sample_size),]
      
      # Create a scatterplot with more informative elements
      plot(sampled_tcp$rerror_rate, sampled_tcp$serror_rate,
           col = rainbow(length(unique(tcp_data$class)))[as.numeric(sampled_tcp$class)],
           pch = 19,
           main = "Error Rate Analysis by Attack Type (TCP)",
           xlab = "Rerror Rate",
           ylab = "Serror Rate",
           cex = 0.7)
      
      # Add a legend
      legend("topright", 
             legend = levels(tcp_data$class),
             col = rainbow(length(unique(tcp_data$class))),
             pch = 19,
             cex = 0.7,
             bty = "n")
    }
  }
} else {
  cat("No TCP data available for analysis.\n")
}
```

### UDP Protocol Analysis

```{r udp_visualizations, fig.width=10, fig.height=6, warning=FALSE, message=FALSE}
# Check if UDP protocol exists and has data
if ("udp" %in% names(protocol_data) && nrow(protocol_data[['udp']]) > 0) {
  udp_data <- protocol_data[['udp']]
  
  # Create a more informative title
  cat("\n### UDP Protocol Analysis", "\n")
  cat("Number of UDP connections:", nrow(udp_data), "\n")
  cat("Number of attack types in UDP:", length(unique(udp_data$class)), "\n\n")
  
  # 1. Attack Distribution in UDP - More informative with percentages
  udp_class_counts <- table(udp_data$class)
  udp_class_df <- data.frame(
    Class = names(udp_class_counts),
    Count = as.numeric(udp_class_counts),
    Percentage = round(100 * as.numeric(udp_class_counts) / sum(udp_class_counts), 2)
  )
  udp_class_df <- udp_class_df[order(udp_class_df$Count, decreasing = TRUE),]
  
  # Print the table for reference
  print(udp_class_df)
  
  # Create a more informative bar plot
  par(mar = c(10, 4, 4, 2) + 0.1)  # Increase bottom margin for labels
  barplot(udp_class_df$Count, 
          names.arg = paste0(udp_class_df$Class, "\n(", udp_class_df$Percentage, "%)"),
          col = rainbow(nrow(udp_class_df)),
          main = "Attack Type Distribution in UDP Protocol",
          ylab = "Count",
          las = 2,  # Rotate labels
          cex.names = 0.8)  # Smaller text for labels
  
  # Add a log scale version for better visibility of smaller classes
  barplot(log10(udp_class_df$Count + 1), 
          names.arg = paste0(udp_class_df$Class, "\n(", udp_class_df$Percentage, "%)"),
          col = rainbow(nrow(udp_class_df)),
          main = "Attack Type Distribution in UDP Protocol (Log Scale)",
          ylab = "Log10(Count + 1)",
          las = 2,
          cex.names = 0.8)
  
  # 2. Service Analysis - Create a more informative pie chart
  udp_service_counts <- table(udp_data$service)
  udp_service_df <- data.frame(
    Service = names(udp_service_counts),
    Count = as.numeric(udp_service_counts),
    Percentage = round(100 * as.numeric(udp_service_counts) / sum(udp_service_counts), 2)
  )
  udp_service_df <- udp_service_df[order(udp_service_df$Count, decreasing = TRUE),]
  
  # Print the table
  cat("\nService Distribution in UDP:\n")
  print(udp_service_df)
  
  # Create a pie chart for top services
  top_n <- min(8, nrow(udp_service_df))
  if (top_n > 0) {
    # Combine small categories into "Other"
    if (nrow(udp_service_df) > top_n) {
      top_services <- udp_service_df[1:top_n,]
      other_services <- data.frame(
        Service = "Other",
        Count = sum(udp_service_df$Count[(top_n+1):nrow(udp_service_df)]),
        Percentage = sum(udp_service_df$Percentage[(top_n+1):nrow(udp_service_df)])
      )
      pie_data <- rbind(top_services, other_services)
    } else {
      pie_data <- udp_service_df
    }
    
    # Create the pie chart
    pie(pie_data$Count, 
        labels = paste0(pie_data$Service, " (", pie_data$Percentage, "%)"),
        col = rainbow(nrow(pie_data)),
        main = "Service Distribution in UDP Protocol")
  }
  
  # 3. Byte Analysis - Create more informative boxplots
  # Check if we have the necessary columns
  if (all(c("src_bytes", "dst_bytes") %in% colnames(udp_data))) {
    # Create side-by-side boxplots for source and destination bytes by class
    par(mfrow = c(1, 2))  # 1 row, 2 columns
    
    # Source bytes boxplot
    boxplot(log10(udp_data$src_bytes + 1) ~ udp_data$class,
            main = "Source Bytes by Attack Type (UDP) - Log Scale",
            xlab = "Attack Type",
            ylab = "Log10(Source Bytes + 1)",
            col = rainbow(length(unique(udp_data$class))),
            las = 2,
            cex.axis = 0.7)
    
    # Destination bytes boxplot
    boxplot(log10(udp_data$dst_bytes + 1) ~ udp_data$class,
            main = "Destination Bytes by Attack Type (UDP) - Log Scale",
            xlab = "Attack Type",
            ylab = "Log10(Destination Bytes + 1)",
            col = rainbow(length(unique(udp_data$class))),
            las = 2,
            cex.axis = 0.7)
    
    # Reset the plotting layout
    par(mfrow = c(1, 1))
  }
  
  # 4. Duration vs Count Analysis - Create a more informative scatterplot
  if (all(c("duration", "count") %in% colnames(udp_data))) {
    # Create a scatterplot with more informative elements
    plot(jitter(udp_data$duration), jitter(udp_data$count),
         col = rainbow(length(unique(udp_data$class)))[as.numeric(udp_data$class)],
         pch = 19,
         main = "Duration vs Connection Count by Attack Type (UDP)",
         xlab = "Duration (jittered)",
         ylab = "Connection Count (jittered)",
         log = "xy",  # Log scale for both axes
         cex = 0.7)
    
    # Add a legend
    legend("topright", 
           legend = levels(udp_data$class),
           col = rainbow(length(unique(udp_data$class))),
           pch = 19,
           cex = 0.7,
           bty = "n")
  }
  
  # 5. Flag Distribution Analysis
  if ("flag" %in% colnames(udp_data)) {
    udp_flag_counts <- table(udp_data$flag)
    udp_flag_df <- data.frame(
      Flag = names(udp_flag_counts),
      Count = as.numeric(udp_flag_counts),
      Percentage = round(100 * as.numeric(udp_flag_counts) / sum(udp_flag_counts), 2)
    )
    udp_flag_df <- udp_flag_df[order(udp_flag_df$Count, decreasing = TRUE),]
    
    # Print the table
    cat("\nFlag Distribution in UDP:\n")
    print(udp_flag_df)
    
    # Create a bar plot
    barplot(udp_flag_df$Count, 
            names.arg = paste0(udp_flag_df$Flag, "\n(", udp_flag_df$Percentage, "%)"),
            col = rainbow(nrow(udp_flag_df)),
            main = "Flag Distribution in UDP Protocol",
            ylab = "Count",
            las = 2)
    
    # Create a contingency table of flag vs class
    flag_class_table <- table(udp_data$flag, udp_data$class)
    
    # Print the table
    cat("\nFlag vs Attack Type in UDP:\n")
    print(flag_class_table)
    
    # Create a mosaic plot
    mosaicplot(flag_class_table,
               main = "Flag vs Attack Type in UDP Protocol",
               color = rainbow(length(unique(udp_data$class))),
               las = 2,
               cex.axis = 0.7)
  }
} else {
  cat("No UDP data available for analysis.\n")
}
```

### ICMP Protocol Analysis

```{r icmp_visualizations, fig.width=10, fig.height=6, warning=FALSE, message=FALSE}
# Check if ICMP protocol exists and has data
if ("icmp" %in% names(protocol_data) && nrow(protocol_data[['icmp']]) > 0) {
  icmp_data <- protocol_data[['icmp']]
  
  # Create a more informative title
  cat("\n### ICMP Protocol Analysis", "\n")
  cat("Number of ICMP connections:", nrow(icmp_data), "\n")
  cat("Number of attack types in ICMP:", length(unique(icmp_data$class)), "\n\n")
  
  # 1. Attack Distribution in ICMP - More informative with percentages
  icmp_class_counts <- table(icmp_data$class)
  icmp_class_df <- data.frame(
    Class = names(icmp_class_counts),
    Count = as.numeric(icmp_class_counts),
    Percentage = round(100 * as.numeric(icmp_class_counts) / sum(icmp_class_counts), 2)
  )
  icmp_class_df <- icmp_class_df[order(icmp_class_df$Count, decreasing = TRUE),]
  
  # Print the table for reference
  print(icmp_class_df)
  
  # Create a more informative bar plot
  par(mar = c(10, 4, 4, 2) + 0.1)  # Increase bottom margin for labels
  barplot(icmp_class_df$Count, 
          names.arg = paste0(icmp_class_df$Class, "\n(", icmp_class_df$Percentage, "%)"),
          col = rainbow(nrow(icmp_class_df)),
          main = "Attack Type Distribution in ICMP Protocol",
          ylab = "Count",
          las = 2,  # Rotate labels
          cex.names = 0.8)  # Smaller text for labels
  
  # 2. Byte Analysis for ICMP
  # Check if we have the necessary columns
  if (all(c("src_bytes", "dst_bytes") %in% colnames(icmp_data))) {
    # Create a scatterplot of source vs destination bytes by attack type
    plot(log10(icmp_data$src_bytes + 1), log10(icmp_data$dst_bytes + 1),
         col = rainbow(length(unique(icmp_data$class)))[as.numeric(icmp_data$class)],
         pch = 19,
         main = "Source vs Destination Bytes by Attack Type (ICMP) - Log Scale",
         xlab = "Log10(Source Bytes + 1)",
         ylab = "Log10(Destination Bytes + 1)",
         cex = 0.7)
    
    # Add a legend
    legend("topright", 
           legend = levels(icmp_data$class),
           col = rainbow(length(unique(icmp_data$class))),
           pch = 19,
           cex = 0.7,
           bty = "n")
    
    # Create side-by-side boxplots for source and destination bytes by class
    par(mfrow = c(1, 2))  # 1 row, 2 columns
    
    # Source bytes boxplot
    boxplot(log10(icmp_data$src_bytes + 1) ~ icmp_data$class,
            main = "Source Bytes by Attack Type (ICMP) - Log Scale",
            xlab = "Attack Type",
            ylab = "Log10(Source Bytes + 1)",
            col = rainbow(length(unique(icmp_data$class))),
            las = 2,
            cex.axis = 0.7)
    
    # Destination bytes boxplot
    boxplot(log10(icmp_data$dst_bytes + 1) ~ icmp_data$class,
            main = "Destination Bytes by Attack Type (ICMP) - Log Scale",
            xlab = "Attack Type",
            ylab = "Log10(Destination Bytes + 1)",
            col = rainbow(length(unique(icmp_data$class))),
            las = 2,
            cex.axis = 0.7)
    
    # Reset the plotting layout
    par(mfrow = c(1, 1))
  }
  
  # 3. Duration Analysis - Create more informative histogram
  if ("duration" %in% colnames(icmp_data)) {
    # Create a histogram of duration by attack type
    hist(log10(icmp_data$duration + 1),
         breaks = 30,
         col = "lightblue",
         main = "Duration Distribution in ICMP Protocol - Log Scale",
         xlab = "Log10(Duration + 1)",
         ylab = "Frequency")
    
    # Create boxplots of duration by attack type
    boxplot(log10(icmp_data$duration + 1) ~ icmp_data$class,
            main = "Connection Duration by Attack Type (ICMP) - Log Scale",
            xlab = "Attack Type",
            ylab = "Log10(Duration + 1)",
            col = rainbow(length(unique(icmp_data$class))),
            las = 2,
            cex.axis = 0.7)
  }
  
  # 4. Flag Distribution Analysis
  if ("flag" %in% colnames(icmp_data)) {
    icmp_flag_counts <- table(icmp_data$flag)
    icmp_flag_df <- data.frame(
      Flag = names(icmp_flag_counts),
      Count = as.numeric(icmp_flag_counts),
      Percentage = round(100 * as.numeric(icmp_flag_counts) / sum(icmp_flag_counts), 2)
    )
    icmp_flag_df <- icmp_flag_df[order(icmp_flag_df$Count, decreasing = TRUE),]
    
    # Print the table
    cat("\nFlag Distribution in ICMP:\n")
    print(icmp_flag_df)
    
    # Create a pie chart
    pie(icmp_flag_df$Count, 
        labels = paste0(icmp_flag_df$Flag, " (", icmp_flag_df$Percentage, "%)"),
        col = rainbow(nrow(icmp_flag_df)),
        main = "Flag Distribution in ICMP Protocol")
    
    # Create a contingency table of flag vs class
    flag_class_table <- table(icmp_data$flag, icmp_data$class)
    
    # Print the table
    cat("\nFlag vs Attack Type in ICMP:\n")
    print(flag_class_table)
    
    # Create a mosaic plot if we have more than one flag type
    if (nrow(flag_class_table) > 1) {
      mosaicplot(flag_class_table,
                 main = "Flag vs Attack Type in ICMP Protocol",
                 color = rainbow(length(unique(icmp_data$class))),
                 las = 2,
                 cex.axis = 0.7)
    }
  }
  
  # 5. Feature importance for distinguishing attack types in ICMP
  # Select numeric columns for analysis
  numeric_cols <- sapply(icmp_data, is.numeric)
  if (sum(numeric_cols) > 2) {
    icmp_numeric <- icmp_data[, numeric_cols]
    
    # Calculate mean values by class
    icmp_means_by_class <- aggregate(icmp_numeric, by = list(Class = icmp_data$class), mean, na.rm = TRUE)
    
    # Calculate overall means
    overall_means <- colMeans(icmp_numeric, na.rm = TRUE)
    
    # Calculate how much each feature varies by class (feature importance proxy)
    feature_variation <- apply(icmp_means_by_class[, -1], 2, function(x) sd(x, na.rm = TRUE) / mean(x, na.rm = TRUE))
    
    # Select top varying features
    top_features <- names(sort(feature_variation, decreasing = TRUE)[1:min(10, length(feature_variation))])
    
    # Print the top features
    cat("\nTop Discriminating Features for ICMP Attacks:\n")
    feature_importance_df <- data.frame(
      Feature = names(feature_variation),
      Variation = feature_variation
    )
    feature_importance_df <- feature_importance_df[order(feature_importance_df$Variation, decreasing = TRUE),]
    print(head(feature_importance_df, 10))
    
    # Plot the top features
    par(mar = c(10, 4, 4, 2) + 0.1)
    barplot(feature_importance_df$Variation[1:min(10, nrow(feature_importance_df))],
            names.arg = feature_importance_df$Feature[1:min(10, nrow(feature_importance_df))],
            col = "skyblue",
            main = "Top Discriminating Features for ICMP Attacks",
            ylab = "Coefficient of Variation",
            las = 2,
            cex.names = 0.7)
  }
} else {
  cat("No ICMP data available for analysis.\n")
}
```

### Cross-Protocol Comparison

```{r cross_protocol_viz, fig.width=10, fig.height=6, message=FALSE}
# Check if we have at least one protocol with data
if (length(names(protocol_data)) > 0) {
  cat("\n### Cross-Protocol Comparison\n")
  cat("Comparing attack patterns across different protocols\n\n")
  
  # 1. Attack type distribution across protocols - using base R for robustness
  # Prepare data frame for attack distribution
  attack_by_protocol <- data.frame(Protocol=character(), 
                                  AttackType=character(), 
                                  Count=numeric(), 
                                  stringsAsFactors=FALSE)
  
  for (protocol in names(protocol_data)) {
    if (nrow(protocol_data[[protocol]]) > 0) {
      # Get class counts for this protocol
      class_counts <- table(protocol_data[[protocol]]$class)
      
      # Filter out any empty class values
      valid_classes <- names(class_counts)[names(class_counts) != ""]
      class_counts <- class_counts[valid_classes]
      
      # Add to data frame
      for (cls in names(class_counts)) {
        if (!is.na(cls) && cls != "") {  # Skip NA or empty class names
          attack_by_protocol <- rbind(
            attack_by_protocol,
            data.frame(
              Protocol = protocol,
              AttackType = cls,
              Count = class_counts[cls],
              stringsAsFactors = FALSE
            )
          )
        }
      }
    }
  }
  
  # Print summary table
  cat("Attack counts by protocol:\n")
  attack_summary <- aggregate(Count ~ Protocol, data = attack_by_protocol, sum)
  print(attack_summary)
  
  # Create a more informative stacked bar plot
  if (nrow(attack_by_protocol) > 0) {
    # Create a contingency table for easier plotting
    attack_table <- xtabs(Count ~ Protocol + AttackType, data = attack_by_protocol)
    
    # Print the table
    cat("\nAttack distribution by protocol:\n")
    print(attack_table)
    
    # Create a stacked barplot
    par(mar = c(10, 4, 4, 2) + 0.1)  # Increase bottom margin for labels
    barplot(t(attack_table), 
            col = rainbow(ncol(attack_table)),
            main = "Attack Type Distribution Across Protocols",
            ylab = "Count",
            legend.text = colnames(attack_table),
            args.legend = list(x = "topright", cex = 0.7))
    
    # Create a percentage version
    attack_pct <- prop.table(attack_table, 1) * 100  # Row percentages
    barplot(t(attack_pct), 
            col = rainbow(ncol(attack_pct)),
            main = "Attack Type Distribution Across Protocols (Percentage)",
            ylab = "Percentage",
            legend.text = colnames(attack_pct),
            args.legend = list(x = "topright", cex = 0.7))
  }
  
  # 2. Feature comparison across protocols
  # Prepare data for comparison
  protocol_comparison <- data.frame()
  
  # Key features to compare
  key_features <- c("duration", "src_bytes", "dst_bytes", "count", "srv_count")
  
  # Calculate summary statistics by protocol
  for (protocol in names(protocol_data)) {
    if (nrow(protocol_data[[protocol]]) > 0) {
      proto_data <- protocol_data[[protocol]]
      
      # Calculate means for numeric features
      for (feature in intersect(key_features, colnames(proto_data))) {
        if (is.numeric(proto_data[[feature]])) {
          protocol_comparison <- rbind(
            protocol_comparison,
            data.frame(
              Protocol = protocol,
              Feature = feature,
              Mean = mean(proto_data[[feature]], na.rm = TRUE),
              Median = median(proto_data[[feature]], na.rm = TRUE),
              StdDev = sd(proto_data[[feature]], na.rm = TRUE),
              stringsAsFactors = FALSE
            )
          )
        }
      }
    }
  }
  
  # Print the comparison table
  if (nrow(protocol_comparison) > 0) {
    cat("\nFeature comparison across protocols:\n")
    print(protocol_comparison)
    
    # Create a bar plot comparing means across protocols
    # Reshape data for easier plotting
    if (length(unique(protocol_comparison$Feature)) > 1) {
      # Create side-by-side barplots for each feature
      features_to_plot <- unique(protocol_comparison$Feature)
      
      for (feature in features_to_plot) {
        feature_data <- protocol_comparison[protocol_comparison$Feature == feature,]
        if (nrow(feature_data) > 0) {
          barplot(feature_data$Mean, 
                  names.arg = feature_data$Protocol,
                  col = rainbow(nrow(feature_data)),
                  main = paste("Mean", feature, "by Protocol"),
                  ylab = paste("Mean", feature),
                  las = 2)
        }
      }
    }
  }
  
  # 3. Service distribution comparison
  service_by_protocol <- data.frame()
  
  for (protocol in names(protocol_data)) {
    if (nrow(protocol_data[[protocol]]) > 0 && "service" %in% colnames(protocol_data[[protocol]])) {
      # Get service counts for this protocol
      service_counts <- table(protocol_data[[protocol]]$service)
      
      # Filter out NA or empty service names
      valid_services <- names(service_counts)[!is.na(names(service_counts)) & names(service_counts) != ""]
      if (length(valid_services) > 0) {
        service_counts <- service_counts[valid_services]
        
        # Get top services
        top_n <- min(5, length(service_counts))
        if (top_n > 0) {
          top_services <- names(sort(service_counts, decreasing = TRUE)[1:top_n])
          
          # Create a temporary data frame for this protocol's services
          temp_df <- data.frame(
            Protocol = rep(protocol, length(top_services)),
            Service = top_services,
            Count = as.numeric(service_counts[top_services]),
            stringsAsFactors = FALSE,
            row.names = NULL  # Explicitly set row.names to NULL to avoid the error
          )
          
          # Append to the main data frame
          service_by_protocol <- rbind(service_by_protocol, temp_df)
        }
      }
    }
  }
  
  # Print the service comparison
  if (nrow(service_by_protocol) > 0) {
    cat("\nTop services by protocol:\n")
    print(service_by_protocol)
    
    # Create a grouped bar plot
    if (length(unique(service_by_protocol$Protocol)) > 1) {
      # Create a contingency table
      service_table <- xtabs(Count ~ Protocol + Service, data = service_by_protocol)
      
      # Plot
      par(mar = c(10, 4, 4, 2) + 0.1)  # Increase bottom margin for labels
      barplot(t(service_table), 
              beside = TRUE,  # Grouped bars
              col = rainbow(ncol(service_table)),
              main = "Top Services by Protocol",
              ylab = "Count",
              las = 2,
              legend.text = colnames(service_table),
              args.legend = list(x = "topright", cex = 0.7))
    }
  }
  
  # 4. Flag distribution comparison
  flag_by_protocol <- data.frame()
  
  for (protocol in names(protocol_data)) {
    if (nrow(protocol_data[[protocol]]) > 0 && "flag" %in% colnames(protocol_data[[protocol]])) {
      # Get flag counts for this protocol
      flag_counts <- table(protocol_data[[protocol]]$flag)
      
      # Filter out NA or empty flag names
      valid_flags <- names(flag_counts)[!is.na(names(flag_counts)) & names(flag_counts) != ""]
      if (length(valid_flags) > 0) {
        flag_counts <- flag_counts[valid_flags]
        
        # Create a temporary data frame for this protocol's flags
        temp_df <- data.frame(
          Protocol = rep(protocol, length(valid_flags)),
          Flag = valid_flags,
          Count = as.numeric(flag_counts[valid_flags]),
          Percentage = 100 * as.numeric(flag_counts[valid_flags]) / sum(flag_counts),
          stringsAsFactors = FALSE,
          row.names = NULL  # Explicitly set row.names to NULL to avoid the error
        )
        
        # Append to the main data frame
        flag_by_protocol <- rbind(flag_by_protocol, temp_df)
      }
    }
  }
  
  # Print the flag comparison
  if (nrow(flag_by_protocol) > 0) {
    cat("\nFlag distribution by protocol:\n")
    print(flag_by_protocol)
    
    # Create a grouped bar plot
    if (length(unique(flag_by_protocol$Protocol)) > 1) {
      # Create a contingency table
      flag_table <- xtabs(Count ~ Protocol + Flag, data = flag_by_protocol)
      
      # Plot
      par(mar = c(10, 4, 4, 2) + 0.1)  # Increase bottom margin for labels
      barplot(t(flag_table), 
              beside = TRUE,  # Grouped bars
              col = rainbow(ncol(flag_table)),
              main = "Flag Distribution by Protocol",
              ylab = "Count",
              las = 2,
              legend.text = colnames(flag_table),
              args.legend = list(x = "topright", cex = 0.7))
      
      # Create a percentage version
      flag_pct <- prop.table(flag_table, 1) * 100  # Row percentages
      barplot(t(flag_pct), 
              beside = TRUE,
              col = rainbow(ncol(flag_pct)),
              main = "Flag Distribution by Protocol (Percentage)",
              ylab = "Percentage",
              las = 2,
              legend.text = colnames(flag_pct),
              args.legend = list(x = "topright", cex = 0.7))
    }
  }
} else {
  cat("No protocol data available for cross-protocol comparison.\n")
}
```
Mean Duration by Protocol: Shows that ICMP has significantly longer duration than TCP and UDP, which is expected since ICMP is often used for ping operations that wait for responses.
Mean src_bytes by Protocol: TCP traffic contains much larger source bytes compared to ICMP and UDP, indicating TCP connections typically transfer more data from client to server.
Mean dst_bytes by Protocol: TCP also dominates in destination bytes, showing TCP connections have substantial two-way data transfer, which aligns with TCP's connection-oriented nature.
Mean count by Protocol: UDP appears to have the highest connection count, suggesting more repeated connections, which is consistent with UDP's stateless nature.

## Additional Protocol-Based Analysis

```{r, message=FALSE, warning=FALSE}
# Source the additional protocol analysis functions
source("additional_protocol_analysis.R")
```

### 1. Attack Type Distribution by Protocol

This visualization shows how different attack types are distributed across protocols, helping us understand which protocols are more susceptible to specific attacks.

```{r attack_by_protocol_plot, fig.width=10, fig.height=6, message=FALSE, warning=FALSE}
# Create attack type distribution plot
attack_plot <- create_attack_by_protocol_plot(balanced_data)
print(attack_plot)
```

### 2. Feature Importance by Protocol

This analysis reveals which features are most important for detecting attacks in each protocol, addressing our research question about protocol-specific feature importance.

```{r feature_importance_by_protocol, fig.width=10, fig.height=8, message=FALSE, warning=FALSE}
# Create feature importance by protocol plot
tryCatch({
  importance_plot <- create_feature_importance_by_protocol(balanced_data, ranger)
  print(importance_plot)
}, error = function(e) {
  cat("Error in feature importance calculation:", e$message, "\n")
  cat("This may be due to factor level issues or insufficient data for some protocols.\n")
})
```

### 3. Service Distribution by Protocol

This visualization shows which services are most commonly used within each protocol, helping identify protocol-specific service vulnerabilities.

```{r service_by_protocol, fig.width=10, fig.height=6, message=FALSE, warning=FALSE}
# Create service distribution by protocol plot
tryCatch({
  service_plot <- create_service_by_protocol(balanced_data)
  print(service_plot)
}, error = function(e) {
  cat("Error in service distribution calculation:", e$message, "\n")
  cat("This may be due to missing service information for some protocols.\n")
})
```

### 4. Attack Success Rate by Protocol

This analysis examines which protocols have higher rates of successful attacks, providing insights into protocol security vulnerabilities.

```{r attack_success_by_protocol, fig.width=10, fig.height=6, message=FALSE, warning=FALSE}
# Create attack success rate by protocol plot
tryCatch({
  success_plot <- create_attack_success_by_protocol(balanced_data)
  if (!is.null(success_plot)) {
    print(success_plot)
  } else {
    cat("Insufficient data to calculate attack success rates by protocol.\n")
  }
}, error = function(e) {
  cat("Error in attack success rate calculation:", e$message, "\n")
})
```

### Key Insights from Protocol-Based Analysis

The protocol-based analysis reveals distinct patterns across TCP, UDP, and ICMP traffic:

1. **TCP Protocol**:
   - Hosts the widest variety of attack types
   - Shows distinctive error rate patterns for different attacks
   - Service distribution is highly diverse
   - Certain features like source/destination bytes and error rates are strong indicators of attack types

2. **UDP Protocol**:
   - More limited range of attack types
   - Different byte transfer patterns than TCP
   - Shorter connection durations on average
   - Flag distributions differ significantly from TCP

3. **ICMP Protocol**:
   - Primarily used for specific attacks like smurf and pod
   - Very distinctive duration patterns
   - Less variability in flag types
   - Byte patterns are highly correlated with attack types

4. **Cross-Protocol Insights**:
   - Different protocols are targeted by different attack types
   - The same attack may exhibit different characteristics across protocols
   - Service vulnerabilities vary significantly by protocol
   - Flag patterns provide strong indicators of attack types within each protocol

These visualizations help identify protocol-specific vulnerabilities and attack signatures, potentially improving our model's ability to detect intrusions in different network contexts. The analysis suggests that protocol-specific models might be more effective than a single model for all traffic types.

```{r}

```

### Key Insights from Protocol-Based Analysis

The protocol-based analysis reveals distinct patterns across TCP, UDP, and ICMP traffic:

1. **TCP Protocol**:
   - Hosts the widest variety of attack types
   - Shows distinctive error rate patterns for different attacks
   - Service distribution is highly diverse

2. **UDP Protocol**:
   - More limited range of attack types
   - Different byte transfer patterns than TCP
   - Shorter connection durations on average

3. **ICMP Protocol**:
   - Primarily used for specific attacks like smurf and pod
   - Very distinctive duration patterns
   - Less variability in flag types

These visualizations help identify protocol-specific vulnerabilities and attack signatures, potentially improving our model's ability to detect intrusions in different network contexts.
