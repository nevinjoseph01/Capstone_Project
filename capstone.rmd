---
title: "Capstone Project"
author: "Abed, Nevin, and Heshan"
date: "2025-04-09"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: true
    toc_depth: 3
    theme: cosmo
    toc_float: true
---


```{r setup, include=FALSE}

if (rstudioapi::isAvailable()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))}

options(width = 200)

# Load libraries ---------------------------------------------------------------
# Create our function "using()" 
using <- \(pkg) {
  # if a package is not installed, install it
  if (!rlang::is_installed(pkg)) {
    install.packages(pkg, repo = "https://cloud.r-project.org")}# load the package
  library(pkg, character.only = TRUE)
}



using("data.table") # The data manipulation king
using("caret") # The ML swiss-knife - http://topepo.github.io/caret/available-models.html
using("plotly") # Beautiful interactive plots
using("ranger") # the fastest and better random forest implementation
using("xgboost") # Extreme Gradient boosting
using("imbalance") # Oversampling
using("ROSE") # Synthetic generation of new data to rebalance
using("VIM")
using("smotefamily")

```


```{r data}
data <- fread("data.csv")

```



```{r}
str(data)
View(data)


```

```{r}
colnames(data)[1:42] <- c(
  "duration", "protocol_type", "service", "flag", "src_bytes", "dst_bytes",
  "land", "wrong_fragt", "urgent", "hot", "num_fail_login", "logged_in",
  "nu_comprom", "root_shell", "su_attempted", "num_root", "nu_file_creat",
  "nu_shells", "nu_access_files", "nu_out_cmd", "is_host_login",
  "is_guest_login", "count", "srv_count", "serror_rate", "srv_serror_rate",
  "rerror_rate", "srv_rerror_rate", "same_srv_rate", "diff_srv_rate",
  "srv_diff_h_rate", "host_count", "host_srv_count", "h_same_sr_rate",
  "h_diff_srv_rate", "h_src_port_rate", "h_srv_d_h_rate", "h_serror_rate",
  "h_sr_serror_rate", "h_rerror_rate", "h_sr_rerror_rate","class"
)

```
```{r}
as.data.frame(table(data$class))
```

```{r}
# Remove all classes with frequency <= 54
class_counts <- table(data$class)
rare_classes <- names(class_counts[class_counts <= 54])
data <- data[!data$class %in% rare_classes]
```


```{r}
any(is.na(data)) # 
colSums(is.na(data)) #
sum(complete.cases(data))
# 22469
nrow(data)
# 528603

```


```{r}
# checking for duplicates:
#sum(duplicated(data)) # 128935
#nrow(data) - sum(duplicated(data))# 399668
#remove duplicates
data <- data[!duplicated(data), ]

```


```{r}
data$class <- as.factor(data$class)
data$protocol_type <- as.factor(data$protocol_type)
data$service <- as.factor(data$service)
data$flag <- as.factor(data$flag)
```

Finding no usefullness to samples with empty class, we decided remove them.
```{r}
data[data$class == "", ] |> head(10)
nrow(data[data$class == "", ]) # 36061
data <- data[data$class != '']
```


```{r}
#imputing:
for (j in names(data)) {
  if(is.numeric(data[[j]])) {
    data[is.na(get(j)), (j) := median(data[[j]], na.rm = TRUE)]}} # median imputation values is done now 

nrow(data) # 399668 after removing duplicates
#View(data)
```

Removing outliers:
Can't remove it right now because the target detection is not binary, but about 26 classes.
The following chunk is disabled for "run above" button
```{r, include=FALSE}
# removing outliers:
training_without_outliers <- copy(data)
numeric_cols <- names(data)[sapply(data, is.numeric)]
for(i in numeric_cols) { 
  for(j in unique(data$class)) {
    values <- data[class == j, get(i)]
    Q1 <- quantile(values, 0.25)
    Q3 <- quantile(values, 0.75)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    training_without_outliers <- training_without_outliers[!(
      class == j & (get(i) < lower_bound | get(i) > upper_bound))]}}

nrow(training_without_outliers)/nrow(data)
# 0.4350536
```


Handling missing char/factor values:
```{r}
# Count empty cells in each column
empty_cells <- sapply(data, function(x) sum(x == ""))
View(as.data.frame(empty_cells))
```




Checking for overlaping values:

```{r}
# Count rows with missing values in each combination of factors
missing_protocol <- data$protocol_type == ""
missing_service <- data$service == ""
missing_flag <- data$flag == ""

# Check total rows with at least one missing value
total_missing_rows <- sum(missing_protocol | missing_service | missing_flag)

# Check overlaps
protocol_service_overlap <- sum(missing_protocol & missing_service)
protocol_flag_overlap <- sum(missing_protocol & missing_flag)
service_flag_overlap <- sum(missing_service & missing_flag)
all_three_missing <- sum(missing_protocol & missing_service & missing_flag)
percent_affected <- (total_missing_rows / nrow(data)) * 100
```

Since removing ~25% of our data would be a substantial loss of information, AI recommends a combined approach:

Analyze the patterns of missingness: Check if the missing values are related to specific classes or patterns in the data. This might give insight into whether the missing data is Missing at Random (MAR) or Missing Not at Random (MNAR).
Mode imputation for rows with a single missing value: For rows where only one of the three variables is missing, use mode imputation.
Consider removal for rows with multiple missing values: For rows where two or all three variables are missing (which appears to be a much smaller subset), consider removal if those patterns don't conform to an identifiable subset of your data.




Checking percentages of missing chars in each feature:
```{r}
empty_cells_percentage <- (empty_cells / nrow(data)) * 100
#print(empty_cells_percentage)
#View(as.data.frame(empty_cells_percentage))

empty_cells_df <- data.frame(
  Column = names(empty_cells),
  Empty_Cells = empty_cells,
  Percentage = empty_cells_percentage
)
# Sort by percentage of empty cells
empty_cells_df <- empty_cells_df[order(-empty_cells_df$Percentage), ]
print(empty_cells_df)



```

```{r}
length(unique(data$flag))
length(unique(data$service))
length(unique(data$protocol_type))
```
> length(unique(data$flag))
[1] 12
> length(unique(data$service))
[1] 67
> length(unique(data$protocol_type))
[1] 4


```{r}
# Create a table of unique values for each feature
unique_values <- sapply(data, function(x) length(unique(x)))
unique_values_df <- data.frame(
  Feature = names(unique_values),
  Unique_Values = unique_values)

# Sort by number of unique values
unique_values_df <- unique_values_df[order(-unique_values_df$Unique_Values), ]

# Print the table
print(unique_values_df)

# View in a more readable format
View(unique_values_df)
```


## Handling Class Imbalance
```{r}
# Set a balanced target for each class
target_samples <- 2000  # We'll aim for 2000 samples per class

# Create a balanced dataset using both undersampling and SMOTE
balanced_data <- data.table()

# First, undersample the majority classes
for (cls in unique(data$class[data$class != ""])) {
  class_data <- data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    # Undersample the majority classes
    set.seed(123)  # For reproducibility
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data <- rbind(balanced_data, balanced_class_data)
  } else {
    # Keep all samples from minority classes
    balanced_data <- rbind(balanced_data, class_data)
  }
}

# Now handle the minority classes using SMOTE
# First convert to data.frame (SMOTE functions expect data.frames)
balanced_df <- as.data.frame(balanced_data)

# We need to convert factor variables to numeric for SMOTE
# Store original factor columns and their levels
factor_cols <- names(balanced_df)[sapply(balanced_df, is.factor)]
factor_levels <- lapply(balanced_df[, factor_cols, drop = FALSE], levels)

# Convert factors to numeric
for (col in factor_cols) {
  balanced_df[[col]] <- as.numeric(balanced_df[[col]])
}

# Apply SMOTE to oversample minority classes
set.seed(123)

# Create a binary classifier for each minority class
for (cls in unique(balanced_data$class[balanced_data$class != ""])) {
  class_count <- sum(balanced_data$class == cls)
  
  if (class_count < target_samples && class_count > 0) {
    # Create a binary classification problem for this class
    temp_df <- balanced_df
    temp_df$target <- ifelse(temp_df$class == as.numeric(factor(cls)), 1, 0)
    
    # Count samples in this class
    n_minority <- sum(temp_df$target == 1)
    
    if (n_minority >= 5) {  # SMOTE needs at least a few samples to work with
      # Determine how many synthetic samples to create
      n_synthetic <- target_samples - n_minority
      
      # Apply SMOTE with dup_size parameter (ratio of synthetic to original samples)
      dup_size <- ceiling(n_synthetic / n_minority)
      
      # Select only the rows for this class to apply SMOTE
      minority_data <- temp_df[temp_df$target == 1, ]
      
      # Only proceed if we have enough samples
      if (nrow(minority_data) >= 5) {
        # Create synthetic samples using SMOTE
        smote_result <- try(SMOTE(
          X = minority_data[, !names(minority_data) %in% c("target", "class")],
          target = minority_data$target,
          K = min(5, nrow(minority_data) - 1),  # Use fewer neighbors if needed
          dup_size = dup_size
        ), silent = TRUE)
        
        if (!inherits(smote_result, "try-error")) {
          # Extract the synthetic samples
          synthetic_samples <- smote_result$syn_data
          
          # Add back the class column
          synthetic_samples$class <- as.numeric(factor(cls))
          
          # Remove the target column if it exists
          if ("target" %in% names(synthetic_samples)) {
            synthetic_samples$target <- NULL
          }
          
          # Limit the number of synthetic samples to what we need
          if (nrow(synthetic_samples) > n_synthetic) {
            synthetic_samples <- synthetic_samples[1:n_synthetic, ]
          }
          
          # Add the synthetic samples to our balanced dataframe
          if (nrow(synthetic_samples) > 0) {
            balanced_df <- rbind(balanced_df[balanced_df$class != as.numeric(factor(cls)), ],
                                rbind(balanced_df[balanced_df$class == as.numeric(factor(cls)), ],
                                     synthetic_samples))
          }
        }
      }
    }
  }
}

# Remove the target column if it exists
if ("target" %in% names(balanced_df)) {
  balanced_df$target <- NULL
}

# Convert back to factors using the original levels
for (col in factor_cols) {
  balanced_df[[col]] <- factor(balanced_df[[col]], levels = 1:length(factor_levels[[col]]), 
                              labels = factor_levels[[col]])
}

# Convert back to data.table
balanced_data <- as.data.table(balanced_df)

# Check the new class distribution
print("Balanced class distribution:")
balanced_class_table <- as.data.frame(table(balanced_data$class))
print(balanced_class_table)

# Verify the total number of samples
cat("Original dataset size:", nrow(data), "\n")
cat("Balanced dataset size:", nrow(balanced_data), "\n")

# Alternative approach: Using simple random oversampling for smaller classes
# This is a simpler approach that might work better in your case

# Create a new data.table for the simpler approach
balanced_data_simple <- data.table()

set.seed(123)  # For reproducibility

for (cls in unique(data$class[data$class != ""])) {
  class_data <- data[class == cls]
  n_samples <- nrow(class_data)
  
  if (n_samples > target_samples) {
    # Undersample the majority classes
    sampled_indices <- sample(1:n_samples, target_samples)
    balanced_class_data <- class_data[sampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples > 0 && n_samples < target_samples) {
    # Random oversampling with replacement for minority classes
    oversampled_indices <- sample(1:n_samples, target_samples, replace = TRUE)
    balanced_class_data <- class_data[oversampled_indices]
    balanced_data_simple <- rbind(balanced_data_simple, balanced_class_data)
  } else if (n_samples == target_samples) {
    # Keep as is if already at target
    balanced_data_simple <- rbind(balanced_data_simple, class_data)
  }
}

# Check the new class distribution using the simple approach
print("Balanced class distribution (simple approach):")
balanced_class_table_simple <- as.data.frame(table(balanced_data_simple$class))
print(balanced_class_table_simple)

# Verify the total number of samples
cat("Balanced dataset size (simple approach):", nrow(balanced_data_simple), "\n")

# Use the simpler balanced dataset for further modeling
balanced_data <- balanced_data_simple
```

Before Balancing:

              Var1   Freq
1                       0
2         b'back.'   2148
3      b'ipsweep.'   1221
4      b'neptune.' 104990
5         b'nmap.'    232
6       b'normal.'  95611
7          b'pod.'    255
8    b'portsweep.'    989
9        b'satan.'   1536
10       b'smurf.' 154486
11    b'teardrop.'    966
12 b'warezclient.'    995



After Balancing:

              Var1 Freq
1                     0
2         b'back.' 2000
3      b'ipsweep.' 2000
4      b'neptune.' 2000
5         b'nmap.' 2000
6       b'normal.' 2000
7          b'pod.' 2000
8    b'portsweep.' 2000
9        b'satan.' 2000
10       b'smurf.' 2000
11    b'teardrop.' 2000
12 b'warezclient.' 2000


```{r}
# Before balancing
before_plot <- ggplot(class_table, aes(x = reorder(Var1, -Freq), y = Freq)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Class Distribution Before Balancing",
       x = "Class", y = "Frequency")

# After balancing
after_plot <- ggplot(balanced_class_table_simple, aes(x = reorder(Var1, -Freq), y = Freq)) +
  geom_bar(stat = "identity", fill = "forestgreen") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Class Distribution After Balancing",
       x = "Class", y = "Frequency")

before_plot
after_plot
```




## Random Forest Model

```{r}

set.seed(42)
train_index <- createDataPartition(balanced_data$class, p = 0.7, list = FALSE)
train_data <- balanced_data[train_index]
test_data <- balanced_data[-train_index]

# Check class distribution in training and testing sets
print("Training set class distribution:")
print(as.data.frame(table(train_data$class)))
print("Testing set class distribution:")
print(as.data.frame(table(test_data$class)))


# Train Random Forest model with ranger
print("Training Random Forest model...")
model_rf <- ranger(
  formula = class ~ ., 
  data = train_data,
  importance = 'impurity',  # For feature importance
  num.trees = 500,          # Number of trees
  mtry = floor(sqrt(ncol(train_data) - 1)), # Default RF formula
  min.node.size = 5,        # Smaller for better precision
  verbose = TRUE            # Track progress
)
```

## Evaluation

```{r}
#print(model_rf)

# --- 2. Making predictions ---
#print("Making predictions on test data...")
pred_rf <- predict(model_rf, data = test_data)
predictions <- pred_rf$predictions

# --- 3. Creating confusion matrix using base R ---
# This avoids the error with caret's confusionMatrix
conf_matrix <- table(Predicted = predictions, Actual = test_data$class)
print("Confusion Matrix:")
print(conf_matrix)

# --- 4. Calculate accuracy manually ---
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)
print(paste("Overall Accuracy:", round(accuracy, 4)))

# --- 5. Calculate per-class metrics ---
# Function to calculate metrics for each class
calculate_class_metrics <- function(conf_mat) {
  n_classes <- nrow(conf_mat)
  class_names <- rownames(conf_mat)
  
  # Create data frame to store metrics
  metrics <- data.frame(
    Class = class_names,
    Precision = numeric(n_classes),
    Recall = numeric(n_classes),
    F1_Score = numeric(n_classes),
    Support = numeric(n_classes)
  )
  
  # Calculate metrics for each class
  for (i in 1:n_classes) {
    class_name <- class_names[i]
    
    # True positives for this class
    tp <- conf_mat[i, i]
    
    # False positives (sum of column - true positives)
    fp <- sum(conf_mat[, i]) - tp
    
    # False negatives (sum of row - true positives)
    fn <- sum(conf_mat[i, ]) - tp
    
    # Calculate metrics
    precision <- if(tp + fp > 0) tp / (tp + fp) else 0
    recall <- if(tp + fn > 0) tp / (tp + fn) else 0
    f1 <- if(precision + recall > 0) 2 * precision * recall / (precision + recall) else 0
    
    # Store in data frame
    metrics$Precision[i] <- precision
    metrics$Recall[i] <- recall
    metrics$F1_Score[i] <- f1
    metrics$Support[i] <- sum(conf_mat[i, ])
  }
  
  return(metrics)
}

# Calculate and display class metrics
class_metrics <- calculate_class_metrics(conf_matrix)
print("Per-class metrics:")
print(class_metrics)

# Calculate macro average (simple average across all classes)
macro_avg <- colMeans(class_metrics[, c("Precision", "Recall", "F1_Score")])
print("Macro Average metrics:")
print(macro_avg)
```

Our Random Forest model shows excellent performance across all network attack classes with the following key observations:

Overall Performance: The model achieves an impressive macro-average F1-score of 0.9139, indicating robust performance across all attack types.
Perfect Classification: Several attack types including 'pod', 'satan', and 'smurf' achieve perfect classification scores (Precision = 1.0, Recall = 1.0), demonstrating the model's strong ability to identify these specific attack patterns.
High Consistency: All attack classes (except for class '0' which has no instances) show F1-scores above 0.98, indicating that the model is reliable across different attack types.
Balanced Precision and Recall: The model maintains a good balance between precision and recall for most classes, suggesting it's not overly biased toward false positives or false negatives.
Class '0' Exception: The empty class ('0') shows no performance metrics as expected since it has no instances in the test set.



